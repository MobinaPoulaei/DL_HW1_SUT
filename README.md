# Deep Learning Course - Sharif University of Technology
### Instructor: Dr. Mahdieh Soleymani

This repository cover fundamental concepts of deep learning, from theoretical foundations to practical implementation using PyTorch. This assignment focuses on foundational concepts, including mathematical derivations and practical implementations of neural networks in PyTorch[cite: 204].


---

### **Theoretical Section**

The theoretical portion of this homework covers fundamental mathematical concepts crucial to deep learning. My handwritten answers are submitted in a single PDF file as required by the course. The topics covered are:

* **Matrix Differentiation**: Derivations for various matrix expressions[cite: 18].
* **Backpropagation**: Step-by-step calculation of gradients for a custom neural network architecture[cite: 46].
* **Optimization**: Analysis of the Backtracking Line Search and Adam optimization algorithms[cite: 72, 99].
* **Regularization**: Concepts related to Lipschitz continuity and the role of Weight Decay[cite: 175, 197].

### **Practical Section**

[cite_start]This part of the homework involves hands-on implementation using PyTorch[cite: 204]. The code is organized into separate files for each question:

* [cite_start]**Basics**: An introduction to PyTorch, where I implemented a simple classification task[cite: 203, 204]. [cite_start]The required files are `Basics.ipynb` and `pytorch_basic.py`[cite: 205].
* [cite_start]**NN Scratch**: Building a complete neural network using custom modules without relying on PyTorch's built-in `nn` library[cite: 208, 209]. [cite_start]The `utils` folder, which was provided, is essential for running this part of the code[cite: 210, 212].
* [cite_start]**Optimization**: A practical assignment on optimization algorithms that was submitted in person[cite: 213, 214].
* [cite_start]**Lazy Gradient (Extra Credit)**: An optional assignment exploring memory challenges when working with neural networks[cite: 217, 218].
