{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e5fd69e"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
        "    <p><b>HW1 @ Deep Learning Course, Dr. Soleymani</b></p>\n",
        "    <p><b>ِDesinged by Payam Taebi</b></p>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ87dPg0aerE"
      },
      "source": [
        "\n",
        "*Full Name:* Mobina Poulaei\n",
        "\n",
        "*Student Number:* 403206962"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIJQvTjfMMDK"
      },
      "source": [
        "\n",
        "\n",
        "# Overview(100 Points)\n",
        "\n",
        "This notebook is primarily for exploration rather than heavy implementation. We aim to examine the performance of various optimization methods on different functions. The submission for this exercise is in-person, so please experiment with the parameters in this section to better understand the challenges of each part.\n",
        "\n",
        "We will define several 1D and 2D functions (e.g., Rastrigin, Schwefel, Rosenbrock, Ackley, Beale, Eggholder, Ill-Conditioned Convex functions) using PyTorch. These functions serve as challenging test cases for our optimizers.\n",
        "\n",
        "Implement different optimization algorithms:\n",
        "We cover first-order methods (SGD, SGD with Momentum, Nesterov Accelerated Gradient, Adagrad, RMSprop, Adam, Nadam) and second-order methods (Newton's Method, L-BFGS). Each method's update rule is implemented and explained.\n",
        "\n",
        "These are some of the components we used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmYX4Q6XKiBi"
      },
      "source": [
        "# First Section\n",
        "\n",
        "In the first section, we have a set of functions for demonstration. Do not modify these; they are implemented to display the functions in this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Upm-lioPFXIO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_1d_function_torch(func, domain, global_x, title=\"1D Function Plot\", initial_x=None, path=None):\n",
        "    \"\"\"\n",
        "    Plot a 1D function (defined in PyTorch) and mark the global minimum.\n",
        "    Optionally, mark an initial point and the optimization path with directional arrows.\n",
        "\n",
        "    Parameters:\n",
        "      - func: a function that accepts a torch tensor and returns a torch tensor.\n",
        "      - domain: tuple (xmin, xmax)\n",
        "      - global_x: x-coordinate of the global minimum.\n",
        "      - title: title for the plot.\n",
        "      - initial_x: (optional) x-coordinate of the initial point.\n",
        "      - path: (optional) a list or numpy array of x-values representing the optimization path.\n",
        "    \"\"\"\n",
        "    x_np = np.linspace(domain[0], domain[1], 1000)\n",
        "    x_torch = torch.tensor(x_np, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        y_torch = func(x_torch)\n",
        "    y_np = y_torch.numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(x_np, y_np, lw=2, label=title)\n",
        "\n",
        "    # Mark initial point if provided.\n",
        "    if initial_x is not None:\n",
        "        init_val = func(torch.tensor(initial_x, dtype=torch.float32)).item()\n",
        "        plt.scatter(initial_x, init_val, color='blue', s=80, zorder=5, label='Initial Point')\n",
        "\n",
        "    # Plot the optimization path if provided.\n",
        "    if path is not None:\n",
        "        path = np.array(path)\n",
        "        # Draw a line connecting the points.\n",
        "        y_path = [func(torch.tensor(p, dtype=torch.float32)).item() for p in path]\n",
        "        plt.plot(path, y_path, color='orange', linewidth=2, marker='o', markersize=4, label='Optimization Path')\n",
        "        # Draw arrows for each segment.\n",
        "        for i in range(len(path)-1):\n",
        "            start = path[i]\n",
        "            end = path[i+1]\n",
        "            y_start = func(torch.tensor(start, dtype=torch.float32)).item()\n",
        "            y_end = func(torch.tensor(end, dtype=torch.float32)).item()\n",
        "            plt.annotate(\"\",\n",
        "                         xy=(end, y_end),\n",
        "                         xytext=(start, y_start),\n",
        "                         arrowprops=dict(arrowstyle=\"->\", color='orange', lw=2))\n",
        "\n",
        "    # Mark the global minimum.\n",
        "    global_val = func(torch.tensor(global_x, dtype=torch.float32)).item()\n",
        "    plt.scatter(global_x, global_val, color='red', s=80, zorder=5, label='Global Minimum')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('f(x)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_2d_contour_torch(func, x_domain, y_domain, global_point, title=\"2D Function Contour\",\n",
        "                            levels=50, cmap='viridis', initial_point=None, path=None):\n",
        "    \"\"\"\n",
        "    Plot a 2D function (defined in PyTorch) using a contour plot, marking the global minimum.\n",
        "    Optionally, mark an initial point and draw the optimization path with arrows indicating direction.\n",
        "\n",
        "    Parameters:\n",
        "      - func: a function that accepts a torch tensor of shape (2, H, W) and returns a tensor of shape (H, W).\n",
        "      - x_domain: tuple (xmin, xmax)\n",
        "      - y_domain: tuple (ymin, ymax)\n",
        "      - global_point: tuple (x*, y*) of the global minimum.\n",
        "      - title: title for the plot.\n",
        "      - levels: number of contour levels.\n",
        "      - cmap: colormap.\n",
        "      - initial_point: (optional) tuple (x0, y0) for the initial point.\n",
        "      - path: (optional) numpy array of shape (n, 2) representing the optimization path.\n",
        "    \"\"\"\n",
        "    x_np = np.linspace(x_domain[0], x_domain[1], 400)\n",
        "    y_np = np.linspace(y_domain[0], y_domain[1], 400)\n",
        "    X_np, Y_np = np.meshgrid(x_np, y_np)\n",
        "\n",
        "    # Create input grid tensor with shape (2, H, W)\n",
        "    grid = torch.tensor(np.stack([X_np, Y_np], axis=0), dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        Z_torch = func(grid)\n",
        "    Z_np = Z_torch.numpy()\n",
        "    vmin, vmax = Z_np.min(), Z_np.max()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cp = plt.contourf(X_np, Y_np, Z_np, levels=levels, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    plt.colorbar(cp)\n",
        "    plt.contour(X_np, Y_np, Z_np, levels=15, colors='black', alpha=0.5)\n",
        "\n",
        "    # Mark initial point if provided.\n",
        "    if initial_point is not None:\n",
        "        plt.scatter(initial_point[0], initial_point[1], color='blue', s=100, marker='o', label='Initial Point')\n",
        "\n",
        "    # Plot the optimization path if provided.\n",
        "    if path is not None:\n",
        "        path = np.array(path)\n",
        "        plt.plot(path[:, 0], path[:, 1], marker='o', color='orange', markersize=4, linewidth=2, label='Optimization Path')\n",
        "        for i in range(len(path) - 1):\n",
        "            start = path[i]\n",
        "            end = path[i+1]\n",
        "            plt.annotate(\"\",\n",
        "                         xy=(end[0], end[1]),\n",
        "                         xytext=(start[0], start[1]),\n",
        "                         arrowprops=dict(arrowstyle=\"->\", color='orange', lw=2))\n",
        "\n",
        "    # Mark the global minimum.\n",
        "    plt.scatter(global_point[0], global_point[1], color='red', s=100, marker='*', label='Global Minimum')\n",
        "\n",
        "    plt.title(title + \" (Contour Plot)\")\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_interactive_3d_torch(func, x_domain, y_domain, global_point, title=\"Interactive 3D Plot\",\n",
        "                              colorscale='viridis', initial_point=None, path=None):\n",
        "    \"\"\"\n",
        "    Create an interactive 3D surface plot using Plotly for a 2D function defined in PyTorch.\n",
        "    Assumes the input is a tensor with two channels: first channel for x and second for y.\n",
        "    Optionally, marks an initial point and draws cones (arrows) to indicate direction along the optimization path.\n",
        "\n",
        "    Parameters:\n",
        "      - func: a function that accepts a torch tensor of shape (2, H, W) and returns a tensor of shape (H, W).\n",
        "      - x_domain: tuple (xmin, xmax)\n",
        "      - y_domain: tuple (ymin, ymax)\n",
        "      - global_point: tuple (x*, y*) for the global minimum.\n",
        "      - title: title for the plot.\n",
        "      - colorscale: Plotly colorscale.\n",
        "      - initial_point: (optional) tuple (x0, y0) for the initial point.\n",
        "      - path: (optional) numpy array of shape (n, 2) representing the optimization path.\n",
        "    \"\"\"\n",
        "    # Create grid for the surface.\n",
        "    x_np = np.linspace(x_domain[0], x_domain[1], 200)\n",
        "    y_np = np.linspace(y_domain[0], y_domain[1], 200)\n",
        "    X_np, Y_np = np.meshgrid(x_np, y_np)\n",
        "\n",
        "    # Create grid tensor with shape (2, H, W)\n",
        "    grid = torch.tensor(np.stack([X_np, Y_np], axis=0), dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        Z_torch = func(grid)\n",
        "    Z_np = Z_torch.numpy()\n",
        "\n",
        "    surface = go.Surface(x=X_np, y=Y_np, z=Z_np, colorscale=colorscale, opacity=0.9, name=title)\n",
        "    data = [surface]\n",
        "\n",
        "    # Optionally add initial point.\n",
        "    if initial_point is not None:\n",
        "        init_val = func(torch.tensor([initial_point[0], initial_point[1]], dtype=torch.float32)).item()\n",
        "        scatter_initial = go.Scatter3d(\n",
        "            x=[initial_point[0]],\n",
        "            y=[initial_point[1]],\n",
        "            z=[init_val],\n",
        "            mode='markers',\n",
        "            marker=dict(size=6, color='blue'),\n",
        "            name='Initial Point'\n",
        "        )\n",
        "        data.append(scatter_initial)\n",
        "\n",
        "    # Optionally add optimization path.\n",
        "    if path is not None:\n",
        "        path = np.array(path)\n",
        "        z_path = []\n",
        "        for pt in path:\n",
        "            z_val = func(torch.tensor([pt[0], pt[1]], dtype=torch.float32)).item()\n",
        "            z_path.append(z_val)\n",
        "        scatter_path = go.Scatter3d(\n",
        "            x=path[:, 0],\n",
        "            y=path[:, 1],\n",
        "            z=z_path,\n",
        "            mode='lines+markers',\n",
        "            line=dict(color='orange', width=4),\n",
        "            marker=dict(size=4, color='orange'),\n",
        "            name='Optimization Path'\n",
        "        )\n",
        "        data.append(scatter_path)\n",
        "\n",
        "        # Add cone traces for directional arrows.\n",
        "        cone_x = []\n",
        "        cone_y = []\n",
        "        cone_z = []\n",
        "        cone_u = []\n",
        "        cone_v = []\n",
        "        cone_w = []\n",
        "\n",
        "        for i in range(len(path) - 1):\n",
        "            start = path[i]\n",
        "            end = path[i+1]\n",
        "            cone_x.append(start[0])\n",
        "            cone_y.append(start[1])\n",
        "            # Evaluate z at start.\n",
        "            z_start = func(torch.tensor([start[0], start[1]], dtype=torch.float32)).item()\n",
        "            cone_z.append(z_start)\n",
        "            cone_u.append(end[0] - start[0])\n",
        "            cone_v.append(end[1] - start[1])\n",
        "            z_end = func(torch.tensor([end[0], end[1]], dtype=torch.float32)).item()\n",
        "            cone_w.append(z_end - z_start)\n",
        "\n",
        "        if len(cone_x) > 0:\n",
        "            cone_trace = go.Cone(\n",
        "                x=cone_x,\n",
        "                y=cone_y,\n",
        "                z=cone_z,\n",
        "                u=cone_u,\n",
        "                v=cone_v,\n",
        "                w=cone_w,\n",
        "                sizemode=\"absolute\",\n",
        "                sizeref=0.5,\n",
        "                anchor=\"tail\",\n",
        "                showscale=False,\n",
        "                colorscale=[[0, 'orange'], [1, 'orange']],\n",
        "                name=\"Step Arrows\"\n",
        "            )\n",
        "            data.append(cone_trace)\n",
        "\n",
        "    # Add global minimum.\n",
        "    global_tensor = torch.tensor([global_point[0], global_point[1]], dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        global_z = func(global_tensor).item()\n",
        "    scatter_global = go.Scatter3d(\n",
        "        x=[global_point[0]],\n",
        "        y=[global_point[1]],\n",
        "        z=[global_z],\n",
        "        mode='markers',\n",
        "        marker=dict(size=8, color='red', symbol='diamond'),\n",
        "        name='Global Minimum'\n",
        "    )\n",
        "    data.append(scatter_global)\n",
        "\n",
        "    fig = go.Figure(data=data)\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        scene=dict(\n",
        "            xaxis_title='x',\n",
        "            yaxis_title='y',\n",
        "            zaxis_title='f(x,y)'\n",
        "        ),\n",
        "        autosize=True\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def plot_loss_vs_epoch(loss_func, path):\n",
        "\n",
        "    \"\"\"\n",
        "    Given a loss function and an optimization path, compute the loss at each epoch and plot it.\n",
        "\n",
        "    Parameters:\n",
        "      - loss_func: A function that accepts a torch tensor representing parameters and returns a scalar loss.\n",
        "      - path: A list or NumPy array of parameter values (e.g. from the optimizer).\n",
        "              Each element should be convertible to a torch tensor.\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    # Loop over each parameter value in the path.\n",
        "    for p in path:\n",
        "        # Convert parameter p to a torch tensor (assume float32).\n",
        "        # p can be a 1D array or a multi-dimensional array.\n",
        "        p_tensor = torch.tensor(p, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            loss_val = loss_func(p_tensor).item()\n",
        "        losses.append(loss_val)\n",
        "\n",
        "    epochs = np.arange(len(losses))\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(epochs, losses, marker='o', color='blue', linewidth=2)\n",
        "    plt.title(\"Loss vs. Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAjw8p8FKqcr"
      },
      "source": [
        "# Next Section (18 Points)\n",
        "\n",
        "In the following section, we examine all the functions, each of which presents its own unique challenge. These functions are both one-dimensional and two-dimensional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smXMurxMlYMW"
      },
      "source": [
        "# 1D Rastrigin Function\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x) = x^2 - 10 \\cos\\left(2\\pi x\\right) + 10\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x \\in [-5,\\, 5] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "\\\\( x = 0 \\\\) with \\\\( f(0)=0 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "This function is **non-convex** and exhibits many local minima due to the cosine modulation. Its oscillatory behavior over \\\\( [-5,5] \\\\) makes it a popular test for global optimization methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj3JwNZya2rZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1D Rastrigin function in PyTorch.\n",
        "def rastrigin_1d_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 1D Rastrigin function using PyTorch operations\n",
        "    return x**2 - 10 * torch.cos(2 * np.pi * x) + 10\n",
        "\n",
        "# Plot the 1D Rastrigin function.\n",
        "plot_1d_function_torch(rastrigin_1d_torch, [-5, 5], global_x=0, title=\"1D Rastrigin Function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_R4yW7ml0KQ"
      },
      "source": [
        "# 1D Schwefel Function\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x) = 418.9829 - x \\sin\\Bigl(\\sqrt{|x|}\\Bigr)\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x \\in [0,\\, 500] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "Approximately at \\\\( x \\approx 420.9687 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "This function is highly **non-convex** and rugged with many deceptive local minima. Its large domain accentuates the difficulty in escaping local traps, making it a challenging benchmark for optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITGCVgusa2tS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1D Schwefel function in PyTorch.\n",
        "def schwefel_1d_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 1D Schwefel function using PyTorch operations\n",
        "    return 418.9829 - x * torch.sin(torch.sqrt(torch.abs(x)))\n",
        "\n",
        "# Plot the 1D Schwefel function.\n",
        "plot_1d_function_torch(schwefel_1d_torch, [0, 500], global_x=420.9687, title=\"1D Schwefel Function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2qGbJ8Nl42Z"
      },
      "source": [
        "# 1D Ill-Conditioned Convex Function\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x) = \\ln\\Bigl(1 + e^{10x}\\Bigr) + x^2\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x \\in [-5,\\, 5] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "Approximately at \\\\( x \\approx -0.3 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "This function is **convex** (and therefore has a unique global minimum), but it is _ill-conditioned_ because the steep exponential part for \\\\( x > 0 \\\\) creates a rapidly changing gradient. It tests how well an optimizer can adjust its step size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1NQwY4sa2vF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1D Ill-Conditioned Convex function in PyTorch.\n",
        "def ill_conditioned_convex_1d_torch(x: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 1D ill-conditioned convex function using PyTorch operations\n",
        "    return torch.log(1 + torch.exp(10*x)) + x**2\n",
        "\n",
        "# Plot the 1D Ill-Conditioned Convex function.\n",
        "plot_1d_function_torch(ill_conditioned_convex_1d_torch, [-5, 5], global_x=-0.3, title=\"1D Ill-Conditioned Convex Function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs4YCMZFmB9v"
      },
      "source": [
        "# Rosenbrock Function (2D)\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x, y) = (1 - x)^2 + 100 \\,(y - x^2)^2\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x \\in [-2,\\, 2] \\\\) and \\\\( y \\in [-1,\\, 3] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "At \\\\( (x,y) = (1,1) \\\\) with \\\\( f(1,1)=0 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "The Rosenbrock function is **non-convex** despite its smoothness. Its narrow, curved valley makes it challenging for gradient-based optimizers to converge to the global minimum. This function is a classical benchmark for testing optimization algorithms.\n",
        "\n",
        "In the next cells, we plot it both as a static contour and as an interactive 3D plot. We'll also mark an initial point (for example, \\\\( (-1,1.5) \\\\)) and the optimization path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzqERVuBmBI5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the Rosenbrock function for 2D inputs in PyTorch.\n",
        "# The input is a tensor of shape (2, H, W) where the first channel is x and the second is y.\n",
        "def rosenbrock_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 2D Rosenbrock function using PyTorch operations\n",
        "    return (1 - X[0])**2 + 100 * (X[1] - X[0]**2)**2\n",
        "\n",
        "# Plot using static contour.\n",
        "plot_2d_contour_torch(rosenbrock_2d_torch, [-2, 2], [-1, 3], global_point=(1, 1),\n",
        "                      title=\"Rosenbrock Function (2D)\")\n",
        "\n",
        "# Plot using interactive 3D.\n",
        "plot_interactive_3d_torch(rosenbrock_2d_torch, [-2, 2], [-1, 3], global_point=(1, 1),\n",
        "                          title=\"Interactive 3D Rosenbrock Function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf-LTGmhYtYI"
      },
      "source": [
        "# 2D Rastrigin Function\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x, y) = x^2 + y^2 - 10 \\Bigl[\\cos\\left(2\\pi x\\right) + \\cos\\left(2\\pi y\\right)\\Bigr] + 20\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x, y \\in [-5,\\, 5] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "At \\\\( (0,0) \\\\) with \\\\( f(0,0)=0 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "The 2D Rastrigin function is highly **non-convex** and multimodal, featuring many local minima. Its oscillatory behavior in both dimensions makes it a demanding test for optimization techniques.\n",
        "\n",
        "Below, we display both the static contour and interactive 3D plots with an example initial point (e.g., \\\\( (3,3) \\\\)) and a dummy optimization path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTCSiYLqYvFR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the 2D Rastrigin function in PyTorch.\n",
        "def rastrigin_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 2D Rastrigin function using PyTorch operations\n",
        "    return X[0]**2 + X[1]**2 - 10 * (torch.cos(2 * np.pi * X[0]) + torch.cos(2 * np.pi * X[1])) + 20\n",
        "\n",
        "# Plot as a static contour.\n",
        "plot_2d_contour_torch(rastrigin_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
        "                      title=\"2D Rastrigin Function\", cmap='plasma')\n",
        "\n",
        "# Plot as an interactive 3D surface.\n",
        "plot_interactive_3d_torch(rastrigin_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
        "                          title=\"Interactive 3D Rastrigin Function\", colorscale='plasma')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQfFakkSYtap"
      },
      "source": [
        "# Ackley Function (2D)\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x, y) = -20 \\exp\\!\\left(-0.2 \\sqrt{\\frac{x^2+y^2}{2}}\\right) - \\exp\\!\\left(\\frac{\\cos\\left(2\\pi x\\right)+\\cos\\left(2\\pi y\\right)}{2}\\right) + 20 + e\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x, y \\in [-5,\\, 5] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "At \\\\( (0,0) \\\\) with \\\\( f(0,0)=0 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "The Ackley function features a nearly flat outer region with a deep, narrow central pit, creating a challenging landscape for optimization algorithms. This function is popular for testing global optimization techniques.\n",
        "\n",
        "We now show both the contour and interactive 3D plots with an initial point (e.g., \\\\( (3,3) \\\\)) and an optimization path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJSmNBCZY55h",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the 2D Ackley function in PyTorch.\n",
        "def ackley_2d_torch(X: torch.Tensor, a=20, b=0.2, c=2*np.pi) -> torch.Tensor:\n",
        "    # TODO: Implement the 2D Ackley function using PyTorch operations\n",
        "    return -a * torch.exp(-b * torch.sqrt(0.5 * (X[0]**2 + X[1]**2))) - torch.exp(0.5 * (torch.cos(c * X[0]) + torch.cos(c * X[1]))) + a + torch.exp(torch.tensor(1))\n",
        "\n",
        "# Plot as a static contour.\n",
        "plot_2d_contour_torch(ackley_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
        "                      title=\"Ackley Function (2D)\", cmap='inferno')\n",
        "\n",
        "# Plot as an interactive 3D surface.\n",
        "plot_interactive_3d_torch(ackley_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
        "                          title=\"Interactive 3D Ackley Function\", colorscale='inferno')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z9KNdHsYtfH"
      },
      "source": [
        "# Beale Function (2D)\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x, y) = \\left(1.5 - x + xy\\right)^2 + \\left(2.25 - x + xy^2\\right)^2 + \\left(2.625 - x + xy^3\\right)^2\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x, y \\in [-4.5,\\, 4.5] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "At \\\\( (3, 0.5) \\\\) with \\\\( f(3,0.5)=0 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "The Beale function is **non-convex** with several local minima, making it a classical benchmark for global optimization. Its complex structure challenges optimizers in 2D space.\n",
        "\n",
        "Below, we provide both a static contour and an interactive 3D plot with an initial point (e.g., \\\\( (0,0) \\\\)) and an optimization path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgxBtD0XZCsJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the 2D Beale function in PyTorch.\n",
        "def beale_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 2D Beale function using PyTorch operations\n",
        "    return (1.5 - X[0] + X[0] * X[1])**2 + (2.25 - X[0] + X[0] * X[1]**2)**2 + (2.625 - X[0] + X[0] * X[1]**3)**2\n",
        "\n",
        "# Plot as a static contour.\n",
        "plot_2d_contour_torch(beale_2d_torch, [-4.5, 4.5], [-4.5, 4.5], global_point=(3, 0.5),\n",
        "                      title=\"Beale Function (2D)\", cmap='Spectral')\n",
        "\n",
        "# Plot as an interactive 3D surface.\n",
        "plot_interactive_3d_torch(beale_2d_torch, [-4.5, 4.5], [-4.5, 4.5], global_point=(3, 0.5),\n",
        "                          title=\"Interactive 3D Beale Function\", colorscale='Spectral')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI84q3nqYthc"
      },
      "source": [
        "# Eggholder Function (2D)\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x, y) = - (y+47) \\sin\\!\\left(\\sqrt{\\left|\\frac{x}{2} + (y+47)\\right|}\\right) - x \\sin\\!\\left(\\sqrt{\\left|x - (y+47)\\right|}\\right)\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x, y \\in [-512,\\, 512] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "Approximately at \\\\( (512,\\, 404.2319) \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "The Eggholder function is extremely **non-convex** with a rugged, highly oscillatory landscape that contains many local minima. It is one of the most challenging test functions in optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1cvpJUtZTTj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the 2D Eggholder function in PyTorch.\n",
        "def eggholder_2d_torch(X: torch.Tensor) -> torch.Tensor:\n",
        "    # TODO: Implement the 2D Eggholder function using PyTorch operations\n",
        "    return - (X[1] + 47) * torch.sin(torch.sqrt(torch.abs((X[0] / 2) + (X[1] + 47)))) - X[0] * torch.sin(torch.sqrt(torch.abs(X[0] - (X[1] + 47))))\n",
        "\n",
        "# Plot as a static contour.\n",
        "plot_2d_contour_torch(eggholder_2d_torch, [-512, 512], [-512, 512], global_point=(512, 404.2319),\n",
        "                      title=\"Eggholder Function (2D)\", cmap='inferno')\n",
        "\n",
        "# Plot as an interactive 3D surface.\n",
        "plot_interactive_3d_torch(eggholder_2d_torch, [-512, 512], [-512, 512], global_point=(512, 404.2319),\n",
        "                          title=\"Interactive 3D Eggholder Function\", colorscale='inferno')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_jSswuwYtkH"
      },
      "source": [
        "# Ill-Conditioned Convex Function (2D)\n",
        "\n",
        "**Mathematical Definition:**\n",
        "\n",
        "$$\n",
        "f(x, y) = 100 \\Bigl(x\\cos\\theta - y\\sin\\theta\\Bigr)^2 + \\Bigl(x\\sin\\theta + y\\cos\\theta\\Bigr)^2 \\quad \\text{with } \\theta = \\frac{\\pi}{6}\n",
        "$$\n",
        "\n",
        "**Domain:**  \n",
        "\\\\( x, y \\in [-5,\\, 5] \\\\)\n",
        "\n",
        "**Global Minimum:**  \n",
        "At \\\\( (0,0) \\\\) with \\\\( f(0,0)=0 \\\\)\n",
        "\n",
        "**Explanation:**  \n",
        "This function is **convex** (thus it has a unique global minimum) but is _ill-conditioned_ due to the rotation and different scaling along the rotated axes. Its behavior tests an optimizer’s ability to deal with steep versus flat directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGdz98dfZhVC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define the 2D Ill-Conditioned Convex function in PyTorch.\n",
        "def ill_conditioned_convex_2d_torch(X: torch.Tensor, theta=torch.tensor(np.pi/6)) -> torch.Tensor:\n",
        "    # TODO: Implement the 2D ill-conditioned convex function using PyTorch operations\n",
        "    return 100*(X[0]*torch.cos(theta) - X[1]*torch.sin(theta))**2 + (X[0]*torch.sin(theta) + X[1]*torch.cos(theta))**2\n",
        "\n",
        "# Plot as a static contour.\n",
        "plot_2d_contour_torch(ill_conditioned_convex_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
        "                      title=\"Ill-Conditioned Convex Function (2D)\", cmap='viridis')\n",
        "\n",
        "# Plot as an interactive 3D surface.\n",
        "plot_interactive_3d_torch(ill_conditioned_convex_2d_torch, [-5, 5], [-5, 5], global_point=(0, 0),\n",
        "                          title=\"Interactive 3D Ill-Conditioned Convex Function (2D)\", colorscale='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoK5omVIK8Ru"
      },
      "source": [
        "# Question (12 Points)\n",
        "\n",
        "For each of the functions presented above, what unique challenges do you think they offer? Consider aspects such as steep gradients, saddle points, local minima, flat regions, and other characteristics that could affect the optimization process.\n",
        "\n",
        "Please provide a brief explanation for each function, describing the potential difficulties in learning or optimizing it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answers\n",
        "---\n",
        "###1D Rastrigin\n",
        "\n",
        "\n",
        "1.   **Many Local Minima**: The function is highly multimodal because of its cosine term. This creates a series of regularly distributed local minima that can easily trap optimization algorithms.\n",
        "\n",
        "2.   **Initialization Sensitivity**: The starting point can have a large effect on the outcome. A poor initialization might land an algorithm in a region dense with local optima, making it challenging to escape.\n",
        "\n",
        "3. **Non-convexity Issues**: The function’s non-convex nature means that standard gradient descent methods can struggle, as they may follow the steepest descent into a local minimum and fail to move out.\n",
        "\n",
        "4. **Oscillatory Gradients**: The periodic cosine component creates oscillatory gradient behavior, which can make step size selection crucial. Too large a step might skip over minima, while too small a step may result in slow convergence."
      ],
      "metadata": {
        "id": "2QhbuMkFDY3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###1D Schwefel\n",
        "1. **Many Local Optima:** The Schwefel function is notorious for its large number of local minima. Even in one dimension, the oscillatory nature of the function creates many deceptive optima, making it challenging for algorithms to identify the true global minimum.\n",
        "\n",
        "2. **Non-Convexity**: The inherent non-convexity of the Schwefel function means that small local changes in the search space do not necessarily lead toward the global optimum. Standard gradient methods may therefore be misled by local landscape features.\n",
        "\n",
        "3. **Steep Gradients**: The sine-based component introduces areas with steep gradients as well as regions where the gradient changes rapidly. This variability makes it difficult for gradient-based algorithms to choose an appropriate step size consistently.\n",
        "\n",
        "4. **Boundary Effects**: A unique challenge for the Schwefel function is that its global optimum is often located near the boundaries of the search space. This means that algorithms must explore the edges effectively or they risk missing the optimum.\n",
        "\n"
      ],
      "metadata": {
        "id": "jqIRYk7yDbJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###1D Ill-Conditioned Convex\n",
        "1. **Steep and Nearly Flat Regions**: Although the function is convex and has a unique minimum, its ill-conditioning implies that there can be regions with extremely steep gradients alongside areas that are nearly flat. This disparity can cause difficulties in selecting an appropriate step size for optimization algorithms.\n",
        "\n",
        "2. **Overshooting in Steep Regions**: Conversely, in regions with very steep gradients, an algorithm might overshoot the optimum if the step size is not carefully controlled, which can destabilize the convergence process.\n"
      ],
      "metadata": {
        "id": "wHojDiVvGSyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###2D Rosenbrock\n",
        "1. **Curved Valley**: The global minimum lies inside a long, narrow, curved valley (often referred to as the \"banana\" shape). This makes it challenging for optimizers to converge quickly because they must navigate along the curved trajectory rather than heading directly toward the minimum.\n",
        "\n",
        "2. **Ill-Conditioning**: The Hessian matrix of the function is badly conditioned near the optimum, leading to slow convergence in optimization methods like Newton’s method.\n",
        "\n",
        "3. **Gradient Behavior**: The gradient is small in the flat regions of the valley and changes rapidly near the curved walls, which challenges the optimizer to balance exploration."
      ],
      "metadata": {
        "id": "AJhxbRBhHaHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2D Rastrigin\n",
        "\n",
        "1. **High Multimodality**: The 2D Rastrigin function is characterized by a vast number of local minima due to its cosine components in both dimensions. This rugged landscape increases the risk of getting trapped in suboptimal solutions.\n",
        "\n",
        "2. **Oscillatory Behavior**: The superimposed cosine terms introduce periodic oscillations in the function's landscape. This creates rapidly changing gradients that can mislead gradient-based methods and complicate step size determination.\n",
        "\n",
        "3. **Dimensional Complexity**: In two dimensions, the coupling between the oscillatory patterns of each axis adds to the complexity. The interactions can create intricate basins of attraction."
      ],
      "metadata": {
        "id": "9H7c8rldKLol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2D Ackley\n",
        "1. **Steep Central Basin**: Near the global optimum, the landscape becomes steeper and more curved. Optimizers must balance careful navigation in these steep regions without overshooting the optimum.\n",
        "\n",
        "2. **Multimodal Landscape**: The presence of many local minima in addition to the global minimum creates a highly rugged and complex landscape. This increases the risk of optimization algorithms getting trapped in suboptimal solutions, particularly for local search methods or gradient-based algorithms.\n",
        "\n",
        "3. **Flat Outer Regions**: The Ackley function has flat regions (especially away from the global optimum) and steep areas near the central optimum. The optimizer needs to be able to escape the flat regions without overshooting the steep, more rapidly changing parts.\n"
      ],
      "metadata": {
        "id": "oSDO3ZBILhL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2D Beale\n",
        "1. **Flat Regions and Plateaus**: Portions of the function can be relatively flat, causing low gradient magnitudes. These flat regions may slow convergence as the optimizer struggles to identify a clear direction for descent.\n",
        "\n",
        "2. **Sensitivity to Initialization**: The success of finding the global optimum is heavily reliant on the starting point. Poor initialization might lead the algorithm into areas where the steep gradients or flat regions hinder effective progress."
      ],
      "metadata": {
        "id": "vO2oT3B7MyYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2D Eggholder\n",
        "1. **Extreme Multimodality**: The Eggholder function is infamous for its large number of local minima. This highly rugged landscape can easily mislead optimization algorithms.\n",
        "\n",
        "2. **Highly Irregular Surface**: The function’s landscape is characterized by deep valleys and high ridges with rapidly changing gradients. Such behavior poses significant difficulties for gradient-based methods, which may struggle with step-size selection.\n",
        "\n",
        "3. **Interdependent Variables**: The variables in the Eggholder function are coupled in a complex way, meaning that the behavior in one dimension directly affects the other. This interdependence increases the difficulty of isolating and correcting errors during optimization."
      ],
      "metadata": {
        "id": "fY_jT7p9OjtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 2D Ill-Conditioned Convex\n",
        "1. **Disparity in Curvature**: The function has a significant difference in curvature along different directions (one direction is much steeper than the other). This disparity creates a narrow valley where the global minimum lies, making it difficult for algorithms to move efficiently.\n",
        "\n",
        "2. **Slow Convergence Along Flat Directions**: In the nearly flat directions, the gradients are very small, which can result in slow progress and require many iterations for convergence.\n",
        "\n",
        "3. **Overshooting in Steep Directions**: The steep direction can lead to overly aggressive updates. Without proper step size control, the optimizer may overshoot the minimum, causing oscillations or divergence.\n",
        "\n",
        "4. **Sensitivity to Scaling and Hyperparameters**: The ill-conditioning means that small changes in the learning rate or other hyperparameters can significantly impact convergence behavior. Proper scaling, adaptive step sizes, or preconditioning techniques are often necessary to achieve efficient optimization."
      ],
      "metadata": {
        "id": "UEh-75u9P77B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5LWqXSXYtmc"
      },
      "source": [
        "# Implementing an Optimization Method (5 Points)\n",
        "\n",
        "Now that we've discussed the general structure of our optimization function, let's explore how to implement a specific optimization method. In this section, we'll walk through the steps required to define an update rule for an optimizer. This involves:\n",
        "\n",
        "1. **Computing the Loss and its Derivatives:**  \n",
        "   We evaluate the loss function for the current parameters, compute the gradient, and optionally calculate the Hessian if using a second-order method.\n",
        "\n",
        "2. **Applying the Update Rule:**  \n",
        "   Based on the computed derivatives, we update the parameters using the specific logic of our chosen optimization method (e.g., adjusting the parameters using SGD, Adam, or Newton's method).\n",
        "\n",
        "3. **Maintaining State:**  \n",
        "   For methods that require momentum or other historical data, we update and maintain a state dictionary that carries this information from one iteration to the next.\n",
        "\n",
        "By following these steps, we can iteratively update the parameters to minimize the loss function and analyze the optimization trajectory. Let's move on to see this process in action.\n",
        "\n",
        "# General Optimization Function Explanation\n",
        "\n",
        "The `optimize` function provides a unified framework for iterative optimization methods. Whether using first-order (gradient-based) or second-order (Hessian-based) methods, the overall process remains similar: we start with an initial guess for the parameters and then iteratively update these parameters in order to minimize a given loss function.\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Initialization:**\n",
        "   - **Parameters:**  \n",
        "     The function receives an initial parameter tensor (`initial_params`). This tensor is cloned and set to require gradients so that we can compute derivatives with respect to it.\n",
        "   - **State Storage:**  \n",
        "     An empty dictionary `state` is initialized to store any additional information required by the update method (e.g., momentum for Adam, or curvature approximations for second-order methods).\n",
        "   - **Path Recording:**  \n",
        "     The initial parameters are stored in a list called `path`. This list will hold the parameter values after each iteration, allowing us to visualize the optimization trajectory later.\n",
        "\n",
        "2. **Iterative Update Process:**\n",
        "   - For each epoch (i.e., optimization step), the following steps occur:\n",
        "     1. **Loss Evaluation:**  \n",
        "        The loss function `loss_func` is evaluated at the current parameter values to determine how well the current parameters perform.\n",
        "     2. **Gradient Computation:**  \n",
        "        The gradient (first derivative) of the loss with respect to the parameters is computed using `loss.backward()`.  \n",
        "        - For **first-order methods**, only the gradient is used.\n",
        "        - For **second-order methods**, additional curvature information is needed.\n",
        "     3. **Hessian Computation (Optional):**  \n",
        "        If the `second_order` flag is set to `True`, the Hessian (matrix of second derivatives) is computed using PyTorch's `torch.autograd.functional.hessian`.  \n",
        "        This Hessian provides insight into the curvature of the loss surface and is useful for methods like Newton's method.\n",
        "     4. **Parameter Update:**  \n",
        "        The update function `update_func` is then called:\n",
        "        - For **first-order methods** (e.g., SGD, Adam):  \n",
        "          It is called with the current parameters, their gradient, the state, and hyperparameters.\n",
        "        - For **second-order methods** (e.g., Newton’s Method, L-BFGS):  \n",
        "          The computed Hessian is also passed.\n",
        "        This function returns updated parameter values and an updated state.\n",
        "     5. **Re-enable Gradient Tracking:**  \n",
        "        After the update, gradients are cleared and re-enabled for the next iteration.\n",
        "     6. **Path Recording:**  \n",
        "        The new parameters are added to the `path` list.\n",
        "\n",
        "3. **Return Value:**\n",
        "   - After completing all epochs, the function returns the `path` — a list of parameter values at each iteration. This path can later be used to visualize the optimization trajectory (for example, by plotting the loss versus epochs or overlaying the path on the loss surface).\n",
        "\n",
        "## Common Underlying Principles\n",
        "\n",
        "- **Unified Structure:**  \n",
        "  All optimization methods follow a similar iterative procedure: compute the loss, compute the gradient (and optionally the Hessian), update the parameters, and record the progress.\n",
        "  \n",
        "- **First-Order vs. Second-Order Methods:**  \n",
        "  - *First-Order Methods* use only the gradient information. Their update rule typically has the form:  \n",
        "  $$\n",
        "    w_{t+1} = w_t - \\eta \\nabla f(w_t)\n",
        "  $$\n",
        "  - *Second-Order Methods* also incorporate curvature information (via the Hessian) to adjust the step direction and size:  \n",
        "    $$ w_{t+1} = w_t - \\left( H(w_t) + \\epsilon I \\right)^{-1} \\nabla f(w_t) $$\n",
        "  The `second_order` flag in our function determines whether the Hessian is computed and passed to the update function.\n",
        "\n",
        "- **State Management:**  \n",
        "  Many advanced optimizers (like Adam, RMSprop, or L-BFGS) maintain a state that includes historical information (e.g., moving averages of gradients, momentum, or curvature approximations). This state is updated at each iteration and passed to the update function to help guide the parameter updates.\n",
        "\n",
        "In summary, the `optimize` function abstracts the common iterative process of optimization, allowing us to plug in different update functions (each corresponding to a different optimization method) and compare their performance on the same problem. This modular approach makes it straightforward to experiment with a wide range of optimization algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q6FpMyx8iRWt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def optimize(loss_func, update_func, initial_params, epochs, hyperparams, second_order=False):\n",
        "    \"\"\"\n",
        "    General optimization function supporting both first-order (gradient-based)\n",
        "    and second-order (Hessian-based) optimization methods.\n",
        "\n",
        "    Parameters:\n",
        "        - loss_func: Function to minimize. Accepts parameters and returns loss.\n",
        "        - update_func: Function that updates parameters.\n",
        "                       Signature (first-order): (params, grad, state, hyperparams) -> (new_params, new_state)\n",
        "                       Signature (second-order): (params, grad, state, hyperparams, hessian) -> (new_params, new_state)\n",
        "        - initial_params: Torch tensor representing the initial parameter values.\n",
        "        - epochs: Number of optimization steps.\n",
        "        - hyperparams: Dictionary of hyperparameters.\n",
        "        - second_order: Boolean flag indicating if second-order derivatives (Hessian) are needed.\n",
        "\n",
        "    Returns:\n",
        "        - path: List of parameter values at each step.\n",
        "    \"\"\"\n",
        "    params = initial_params.clone().detach().requires_grad_(True)\n",
        "    state = {}  # Store state (e.g., momentum, Hessian approximations, etc.)\n",
        "    path = [params.clone().detach().numpy()]  # Store initial point\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss = loss_func(params)  # Compute loss\n",
        "        loss.backward()  # Compute gradient\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient tracking during updates\n",
        "            if second_order:\n",
        "                # TODO: Compute the Hessian matrix using torch.autograd.functional.hessian\n",
        "                hessian = torch.autograd.functional.hessian(loss_func, params)\n",
        "                # TODO: Update parameters using the update_func with second-order information\n",
        "                params, state = update_func(params, params.grad, state, hyperparams, hessian)\n",
        "            else:\n",
        "                # TODO: Update parameters using the update_func with first-order information\n",
        "                params, state = update_func(params, params.grad, state, hyperparams)\n",
        "\n",
        "            params.requires_grad_(True)\n",
        "\n",
        "        path.append(params.clone().detach().numpy())\n",
        "\n",
        "    return path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIN2LRKlM3IK"
      },
      "source": [
        "# Optimization Demonstration Functions\n",
        "\n",
        "Next, we have a set of functions that display the optimization process by running it on all the functions. You may edit them if needed for better visualization, but preferably, do not modify these two sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-PDjwmdrmYFm"
      },
      "outputs": [],
      "source": [
        "def optimize_and_plot(loss_func, update_func, initial_params, epochs, hyperparams, x_domain, y_domain=None, title=\"Optimization\" ,second_order=False):\n",
        "    \"\"\"\n",
        "    Perform optimization and automatically visualize the results for 1D or 2D functions.\n",
        "\n",
        "    Parameters:\n",
        "      - loss_func: Function to minimize. It accepts parameters and returns a scalar loss.\n",
        "      - update_func: Function that updates parameters. Signature: (params, grad, state, hyperparams) -> (new_params, new_state)\n",
        "      - initial_params: Torch tensor representing the initial parameter values.\n",
        "      - epochs: Number of optimization steps.\n",
        "      - hyperparams: Dictionary of hyperparameters for the update function.\n",
        "      - x_domain: Tuple (xmin, xmax) defining the domain for plotting.\n",
        "      - y_domain: Tuple (ymin, ymax) defining the domain for 2D functions (set to None for 1D).\n",
        "      - title: Title for the plots.\n",
        "\n",
        "    Returns:\n",
        "      - path: The list of parameter values at each epoch (for further processing if desired).\n",
        "    \"\"\"\n",
        "    # Run the optimization (assumes a separate \"optimize\" function exists).\n",
        "    path = optimize(loss_func, update_func, initial_params, epochs, hyperparams , second_order=second_order)\n",
        "    path_np = np.array(path)  # Convert list of torch tensors (or arrays) to a NumPy array.\n",
        "    initial_np = initial_params.detach().numpy()  # Get initial point as a NumPy array.\n",
        "\n",
        "    # Check dimensionality: if initial_params has shape (1, ...) then it's 1D; if (2, ...) then 2D.\n",
        "    if initial_params.shape[0] == 1:\n",
        "        # 1D case: Pass initial_x and path.\n",
        "        plot_1d_function_torch(\n",
        "            func=loss_func,\n",
        "            domain=x_domain,\n",
        "            global_x=path_np[-1][0],\n",
        "            title=title,\n",
        "            initial_x=initial_np[0],\n",
        "            path=path_np\n",
        "        )\n",
        "        plot_loss_vs_epoch(loss_func, path_np)\n",
        "    elif initial_params.shape[0] == 2:\n",
        "        # 2D case: Pass initial_point as a tuple and the full path.\n",
        "        plot_2d_contour_torch(\n",
        "            func=loss_func,\n",
        "            x_domain=x_domain,\n",
        "            y_domain=y_domain,\n",
        "            global_point=(path_np[-1][0], path_np[-1][1]),\n",
        "            title=title,\n",
        "            initial_point=(initial_np[0], initial_np[1]),\n",
        "            path=path_np\n",
        "        )\n",
        "        plot_interactive_3d_torch(\n",
        "            func=loss_func,\n",
        "            x_domain=x_domain,\n",
        "            y_domain=y_domain,\n",
        "            global_point=(path_np[-1][0], path_np[-1][1]),\n",
        "            title=title,\n",
        "            initial_point=(initial_np[0], initial_np[1]),\n",
        "            path=path_np\n",
        "        )\n",
        "        plot_loss_vs_epoch(loss_func, path_np)\n",
        "    else:\n",
        "        raise ValueError(\"Function must be either 1D or 2D.\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NZ84cDiT-efX"
      },
      "outputs": [],
      "source": [
        "def run_all_optimizations(update_func, hyperparams, epochs, second_order=False):\n",
        "    \"\"\"\n",
        "    Run optimization for all benchmark functions (both 1D and 2D) using the provided update function,\n",
        "    hyperparameters, and epoch count. It calls `optimize_and_plot` to handle visualization.\n",
        "\n",
        "    Parameters:\n",
        "      - update_func: The update function to use (e.g., adam_update, sgd_update, etc.).\n",
        "      - hyperparams: Dictionary of hyperparameters for the update function.\n",
        "      - epochs: Number of optimization steps to perform.\n",
        "    \"\"\"\n",
        "    # List of benchmark function configurations.\n",
        "    configs = [\n",
        "        # 1D Functions\n",
        "        {\n",
        "            \"title\": \"1D Rastrigin Function Optimization\",\n",
        "            \"loss_func\": rastrigin_1d_torch,\n",
        "            \"initial_params\": torch.tensor([3.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-5, 5),\n",
        "            \"y_domain\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"1D Schwefel Function Optimization\",\n",
        "            \"loss_func\": schwefel_1d_torch,\n",
        "            \"initial_params\": torch.tensor([100.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (0, 500),\n",
        "            \"y_domain\": None\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"1D Ill-Conditioned Convex Function Optimization\",\n",
        "            \"loss_func\": ill_conditioned_convex_1d_torch,\n",
        "            \"initial_params\": torch.tensor([3.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-5, 5),\n",
        "            \"y_domain\": None\n",
        "        },\n",
        "        # 2D Functions\n",
        "        {\n",
        "            \"title\": \"2D Rosenbrock Function Optimization\",\n",
        "            \"loss_func\": rosenbrock_2d_torch,\n",
        "            \"initial_params\": torch.tensor([-1.0, 1.5], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-2, 2),\n",
        "            \"y_domain\": (-1, 3)\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"2D Rastrigin Function Optimization\",\n",
        "            \"loss_func\": rastrigin_2d_torch,\n",
        "            \"initial_params\": torch.tensor([3.0, 3.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-5, 5),\n",
        "            \"y_domain\": (-5, 5)\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"2D Ackley Function Optimization\",\n",
        "            \"loss_func\": ackley_2d_torch,\n",
        "            \"initial_params\": torch.tensor([3.0, 3.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-5, 5),\n",
        "            \"y_domain\": (-5, 5)\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"2D Beale Function Optimization\",\n",
        "            \"loss_func\": beale_2d_torch,\n",
        "            \"initial_params\": torch.tensor([-3.0, 3.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-4.5, 4.5),\n",
        "            \"y_domain\": (-4.5, 4.5)\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"2D Eggholder Function Optimization\",\n",
        "            \"loss_func\": eggholder_2d_torch,\n",
        "            \"initial_params\": torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-512, 512),\n",
        "            \"y_domain\": (-512, 512)\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"2D Ill-Conditioned Convex Function Optimization\",\n",
        "            \"loss_func\": ill_conditioned_convex_2d_torch,\n",
        "            \"initial_params\": torch.tensor([4.0, -3.0], dtype=torch.float32, requires_grad=True),\n",
        "            \"x_domain\": (-5, 5),\n",
        "            \"y_domain\": (-5, 5)\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Iterate over each function and call optimize_and_plot\n",
        "    for config in configs:\n",
        "        print(\"Optimizing:\", config[\"title\"])\n",
        "\n",
        "        optimize_and_plot(\n",
        "            loss_func=config[\"loss_func\"],\n",
        "            update_func=update_func,\n",
        "            initial_params=config[\"initial_params\"],\n",
        "            epochs=epochs,\n",
        "            hyperparams=hyperparams,\n",
        "            x_domain=config[\"x_domain\"],\n",
        "            y_domain=config[\"y_domain\"],\n",
        "            title=config[\"title\"],\n",
        "            second_order=second_order\n",
        "        )\n",
        "\n",
        "        print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsdVw8NLNFuC"
      },
      "source": [
        "# Implementation of Optimization Algorithms\n",
        "\n",
        "Now that the optimization functions have been implemented, in this section we will implement various optimization algorithms. Some parts of these algorithms need to be implemented by you, while other parts have already been provided.\n",
        "\n",
        "For each algorithm, experiment with different parameters and fine-tune them to achieve the best possible performance. The selection and tuning of these parameters are part of your grade, and the number of times you experiment with various settings is a crucial component of this exercise.\n",
        "\n",
        "After implementing each algorithm, please submit a report detailing your observations and findings. Make sure to document your parameter tuning process and the performance outcomes for each algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZTlCjY0E8oR"
      },
      "source": [
        "# Stochastic Gradient Descent (SGD)(5 Points)\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\eta \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
        "- \\\\(\\eta\\\\) is the learning rate, and\n",
        "- \\\\(\\nabla f(w_t)\\\\) is the gradient of the loss function evaluated at \\\\(w_t\\\\).\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Stochastic Gradient Descent (SGD) is a fundamental first-order optimization method that updates the parameters in the opposite direction of the gradient. At each iteration, the update rule moves the parameters by a small step proportional to the gradient and the learning rate. This method is widely used because of its simplicity and efficiency, particularly when dealing with large datasets. However, its performance heavily depends on the proper tuning of the learning rate; if set too high, the algorithm might overshoot minima, and if too low, convergence can be very slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9CR_FpRrsghO"
      },
      "outputs": [],
      "source": [
        "def sgd_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    SGD update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to the parameters.\n",
        "      - state: Dictionary to store any state (unused in SGD).\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain \"lr\" (learning rate).\n",
        "      - **kwargs: Additional arguments (ignored for SGD).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameters.\n",
        "      - state: Unchanged state (empty dictionary).\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]  # Get the learning rate from hyperparameters.\n",
        "\n",
        "    # TODO: Implement the standard SGD update rule\n",
        "    new_params = params - lr*grad\n",
        "\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
        "sgd_hyperparams = {\"lr\": 0.1}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD.\n",
        "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UJA4jZgTbAde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
        "sgd_hyperparams = {\"lr\": 0.05}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD.\n",
        "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "InqRebiHZ-wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGXN81GJFHIu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
        "sgd_hyperparams = {\"lr\": 0.01}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD.\n",
        "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
        "sgd_hyperparams = {\"lr\": 0.001}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD.\n",
        "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5vEnVzKHa-zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
        "sgd_hyperparams = {\"lr\": 0.0001}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD.\n",
        "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dArh25vqa-2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD (adjust the learning rate as needed).\n",
        "sgd_hyperparams = {\"lr\": 0.00001}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD.\n",
        "run_all_optimizations(update_func=sgd_update, hyperparams=sgd_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0bCGIi1Sa-5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESRhsIjkOjYs"
      },
      "source": [
        "# SGD Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the SGD optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    sgd_hyperparams = {\"lr\": 0.01}\n",
        "\n",
        "In this section, you are required to test various learning rates and analyze how they affect the optimizer's performance. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the learning rate values you tested. Provide a clear list or table of these values.\n",
        "\n",
        "###✅ Answer\n",
        "| Learning Rate |\n",
        "|--------------|\n",
        "| 0.1          |\n",
        "| 0.05         |\n",
        "| 0.01         |\n",
        "| 0.001        |\n",
        "| 0.0001       |\n",
        "| 0.00001      |\n",
        "\n",
        "\n",
        "----\n",
        "2. **Performance Analysis:**\n",
        "   - How did different learning rates impact the convergence behavior of SGD?\n",
        "   - Were there any specific learning rates that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested learning rates cause the optimizer to get stuck in local minima?\n",
        "\n",
        "###✅ Answer\n",
        "Based on my observations, different learning rates significantly impact the convergence behavior of Stochastic Gradient Descent (SGD).\n",
        "\n",
        "A higher learning rate (like 0.1, 0.01) tends to cause instability, overshooting, or oscillations, especially in functions with sharp gradients or complex landscapes like Rosenbrock, Beale, and Ill-conditioned convex. Some functions, such as Rastrigin, show fluctuations, indicating that the step size is too large to settle into a minimum. In other cases, like Schwefel, Ackley, and Eggholder, the optimizer gets trapped in local minima, suggesting that high LR jumps too aggressively and fails to explore better solutions.\n",
        "\n",
        "A lower learning rate (0.001, 0.0001) leads to extremely slow convergence. Many functions, such as Rastrigin, Schwefel, and Ackley, result in the optimizer getting stuck near the initial point, meaning the step size is too small to make meaningful progress. However, in well-conditioned convex landscapes, such as Beale and Ill-conditioned convex, this small LR allows steady progress and reaches the global minimum.\n",
        "\n",
        "The results suggest that a moderate learning rate (e.g., 0.01 or 0.001) might provide a balance between speed and stability. A too-high LR risks instability, while a too-low LR may cause stagnation.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Optimal Learning Rate:**\n",
        "   - Which learning rate, among those tested, achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider this learning rate optimal based on your observations.\n",
        "\n",
        "###✅ Answer\n",
        "The optimal choice is in the range of 0.01 or 0.001. Because these values are small enough to reduce instability and overshooting. And they are large enough to allow meaningful updates without stagnation. Many optimization problems, particularly those with a mix of smooth and rugged landscapes, tend to perform well within this range.\n",
        "\n",
        "---\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying the learning rate. For instance, did higher learning rates lead to faster initial convergence at the risk of overshooting, or did lower learning rates provide more stable but slower convergence?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "###✅ Answer\n",
        "Based on my observations, adjusting the learning rate introduces clear trade-offs between convergence speed and stability:\n",
        "\n",
        "- **Higher Learning Rate (0.1) – Fast Convergence, but Risky**\n",
        "\n",
        "  - **Faster initial convergence**: In some cases, like 2D Beale and 2D Ill-conditioned convex, the optimizer moved rapidly but overshot.\n",
        "  - **Overshooting & instability**: Functions like 2D Rosenbrock and 2D Ill-conditioned convex exhibited large oscillations, making it difficult for SGD to settle into an optimal solution.\n",
        "  - **Fluctuations in rugged landscapes**: In highly non-convex functions like Rastrigin, the optimizer fluctuated, likely bouncing between local minima without settling.\n",
        "\n",
        "- **Lower Learning Rate (0.0001) – Stable but Extremely Slow**\n",
        "\n",
        "  - **Minimal movement in early iterations**: Many functions (e.g., 1D Rastrigin, 1D Schwefel, 2D Rosenbrock, and 2D Ackley) resulted in the optimizer being stuck near the initial point, suggesting that the step size was too small to make meaningful progress.\n",
        "  - **Improved stability for smooth functions**: In convex landscapes (2D Beale, 2D Ill-conditioned convex), the optimizer successfully reached the global minimum without overshooting.\n",
        "  - **Local minima trapping**: For rugged functions like Ackley and Eggholder, the small LR prevented the optimizer from escaping poor local solutions.\n",
        "\n",
        "- **Emerging Patterns and Trends**\n",
        "  - Higher LRs accelerate learning but at the cost of stability, often leading to oscillations or divergence.\n",
        "  - Lower LRs ensure stability but can be too slow, requiring more iterations to reach an optimal solution.\n",
        "  - Non-convex functions (e.g., Rastrigin, Eggholder, Ackley) are highly sensitive to LR. A high LR causes erratic behavior, while a low LR leads to stagnation.\n",
        "  - Smooth convex functions (e.g., Beale, Ill-conditioned convex) benefit from a lower LR, as slow and steady updates help reach the global minimum.\n",
        "---\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate when using SGD in similar optimization tasks?\n",
        "\n",
        "###✅ Answer\n",
        "- Increase the rate slightly if the landscape is rugged or non-convex, but be cautious of overshooting.\n",
        "- Use lower rates for smooth or convex functions to ensure convergence without instability.\n",
        "- Consider learning rate schedules or adaptive methods to fine-tune the learning rate over time.\n",
        "---\n",
        "###⏩ Conclusion\n",
        "\n",
        "| Learning Rate | Convergence Speed | Stability           | Behavior                                               |\n",
        "|---------------|-------------------|---------------------|--------------------------------------------------------|\n",
        "| 0.1           | Fast              | Unstable            | Overshooting, oscillations, fluctuations, and local minima trapping |\n",
        "| 0.05          | Moderate          | Unstable            | Similar to 0.1, but slightly better stability in some cases, still prone to oscillations |\n",
        "| 0.01          | Moderate          | More Stable         | May cause overshooting in some functions, but generally smooth convergence |\n",
        "| 0.001         | Slow              | Stable              | Very slow convergence, but smoother behavior, may get stuck near initial point |\n",
        "| 0.0001        | Very Slow         | Very Stable         | Slow, stuck near initial point for most functions, may reach global minima in smooth landscapes |\n",
        "| 0.00001       | Extremely Slow    | Very Stable         | Almost no progress for most functions, may only converge in well-conditioned convex landscapes |\n",
        "\n",
        "Make sure your report includes detailed observations, a comparative analysis, and a summary of all the learning rates tested. Your analysis should clearly illustrate how the choice of learning rate influences the performance of the SGD optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sldK0pfHNBm"
      },
      "source": [
        "# SGD with Momentum (5 Points)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "The SGD with Momentum update is given by:\n",
        "\n",
        "$$\n",
        "v_t = \\beta v_{t-1} + \\eta \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - v_t\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),  \n",
        "- \\\\(v_t\\\\) is the velocity (accumulated momentum) at iteration \\\\(t\\\\),  \n",
        "- \\\\(\\eta\\\\) is the learning rate, and  \n",
        "- \\\\(\\beta\\\\) is the momentum coefficient (typically between 0 and 1).\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "SGD with Momentum introduces an additional term \\\\(v_t\\\\) that accumulates a running average of past gradients. This helps dampen oscillations and accelerates convergence, especially in scenarios where the gradient direction is consistent. However, careful tuning of both the learning rate \\\\(\\eta\\\\) and the momentum coefficient \\\\(\\beta\\\\) is necessary for optimal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "umXqXtHbF4CJ"
      },
      "outputs": [],
      "source": [
        "def momentum_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    SGD with Momentum update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to the parameters.\n",
        "      - state: Dictionary to store state (e.g., velocity). Initially, this can be empty.\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "          \"lr\": learning rate, and\n",
        "          \"momentum\": momentum coefficient.\n",
        "      - **kwargs: Additional arguments (not used here).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameters.\n",
        "      - state: Updated state dictionary (contains the velocity).\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]\n",
        "    momentum = hyperparams[\"momentum\"]\n",
        "\n",
        "    if \"v\" not in state:\n",
        "        # TODO: Initialize the velocity tensor with zeros, having the same shape as params\n",
        "        state[\"v\"] = torch.zeros_like(params)\n",
        "\n",
        "    # TODO: Implement the momentum update rule: v_t = momentum * v_{t-1} + lr * grad\n",
        "    state[\"v\"] = momentum*state[\"v\"] + lr*grad\n",
        "\n",
        "    # TODO: Update parameters using the velocity term\n",
        "    new_params = params - state[\"v\"]\n",
        "\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.1,       # learning rate\n",
        "    \"momentum\": 0.9    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lGxm16KNbwjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnUgXIssF6Ol",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.01,       # learning rate\n",
        "    \"momentum\": 0.9    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.001,       # learning rate\n",
        "    \"momentum\": 0.9    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "46l7NB-tbwoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.01,       # learning rate\n",
        "    \"momentum\": 0.95    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uLUORdibbwlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.001,       # learning rate\n",
        "    \"momentum\": 0.95    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eMXuK31dbwwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.01,       # learning rate\n",
        "    \"momentum\": 0.8    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QYPrw2916vu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for SGD with Momentum.\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.001,       # learning rate\n",
        "    \"momentum\": 0.8    # momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using SGD with Momentum.\n",
        "run_all_optimizations(update_func=momentum_update, hyperparams=momentum_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q38aHcfzLzO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeaRKyEDOubj"
      },
      "source": [
        "# Momentum Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the Momentum optimizer, the hyperparameters are defined as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "momentum_hyperparams = {\n",
        "    \"lr\": 0.001,       # learning rate\n",
        "    \"momentum\": 0.9    # momentum coefficient\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "In this section, you are required to test various values for both the learning rate and the momentum coefficient, and analyze their impact on the optimizer's performance. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. Parameter Variation:\n",
        "   - List all the learning rate values you tested.\n",
        "   - List all the momentum coefficient values you tested.\n",
        "   - Provide a clear list or table of these values along with any combinations you experimented with.\n",
        "\n",
        "### ✅ Answer\n",
        "| Learning Rate | Momentum Coefficient |\n",
        "|--------------|----------------------|\n",
        "| 0.1          | 0.9                  |\n",
        "| 0.01         | 0.9                  |\n",
        "| 0.001        | 0.9                  |\n",
        "| 0.01         | 0.95                 |\n",
        "| 0.001        | 0.95                 |\n",
        "| 0.01         | 0.8                  |\n",
        "| 0.001         | 0.8                  |\n",
        "\n",
        "\n",
        "---\n",
        "2. Performance Analysis:\n",
        "   - How did changes in the learning rate affect the convergence behavior of the Momentum optimizer?\n",
        "   - How did variations in the momentum coefficient influence the optimization process?\n",
        "   - Were there specific combinations that led to faster convergence or instability (e.g., overshooting or slow progress)?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Higher Learning Rate (0.1):\n",
        "\n",
        "    - Led to instability and overshooting in multiple cases.\n",
        "    - The optimizer struggled to settle into a minimum, often oscillating.\n",
        "\n",
        "- Moderate Learning Rate (0.01):\n",
        "\n",
        "  - Improved stability compared to 0.1 but still showed overshooting in some cases.\n",
        "  - Works well with certain momentum values but needs careful tuning.\n",
        "\n",
        "- Lower Learning Rate (0.001):\n",
        "\n",
        "  - Resulted in slow but stable convergence.\n",
        "  - In some cases, reached the global minimum without overshooting.\n",
        "\n",
        "- Higher Momentum (0.95):\n",
        "\n",
        "  - Increased the risk of overshooting, particularly at 0.01 learning rate.\n",
        "  - Helped in faster initial progress but sometimes made convergence unstable.\n",
        "\n",
        "- Moderate Momentum (0.9):\n",
        "  - Balanced speed and stability better than 0.95.\n",
        "  - However, at a high learning rate (0.1), it still showed overshooting.\n",
        "\n",
        "- Lower Momentum (0.8):\n",
        "\n",
        "  - Reduced instability but slowed down convergence.\n",
        "  - Worked better at 0.001 learning rate, but was not optimal for all cases.\n",
        "---\n",
        "\n",
        "3. Optimal Parameter Combination:\n",
        "   - Which combination of learning rate and momentum coefficient achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider this combination optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Faster Convergence Compared to Lower Learning Rates\n",
        "\n",
        "  - Unlike LR = 0.001, which was very stable but too slow, LR = 0.01 reached lower loss values faster.\n",
        "  - It avoided getting stuck near the initial point, which was common with very low learning rates.\n",
        "\n",
        "- More Stability Compared to Higher Learning Rates\n",
        "\n",
        " - Higher learning rates like 0.1 often caused overshooting and oscillations, making convergence unpredictable.\n",
        " - LR = 0.01 reduced these fluctuations while still maintaining relatively fast updates.\n",
        "\n",
        "- Momentum = 0.9 Helped Maintain Smooth Descent\n",
        "\n",
        "  - Momentum 0.9 provided enough acceleration to avoid slow progress, especially in functions with plateaus.\n",
        "  - Unlike momentum 0.95, which frequently led to overshooting, 0.9 kept updates controlled.\n",
        "  - Lower momentum values like 0.8 resulted in slower progress, especially in rugged landscapes.\n",
        "\n",
        "---\n",
        "4. Trade-offs and Observations:\n",
        "   - Discuss any trade-offs observed when tuning these parameters. For example, did a higher momentum help accelerate convergence at the risk of overshooting, or did a lower momentum lead to more stable but slower updates?\n",
        "   - Describe any trends or patterns that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- **Effect of Learning Rate**: A high learning rate led to rapid progress but often caused instability, overshooting, or divergence, especially in non-convex landscapes. While it helped escape shallow regions, it struggled to settle into an optimal solution. In contrast, a low learning rate ensured stable and precise updates but significantly slowed convergence, requiring more iterations to reach the minimum.\n",
        "\n",
        "- **Effect of Momentum**: High momentum accelerated convergence by maintaining velocity, making it effective for escaping plateaus. However, it also increased the risk of overshooting and oscillations, especially with a high learning rate. Lower momentum provided more stability by reducing oscillations, but at the cost of slower convergence, making it less effective for navigating complex loss surfaces.\n",
        "\n",
        "---\n",
        "5. Recommendations:\n",
        "   - Based on your experimental results, what recommendations would you give for selecting learning rate and momentum values when using momentum-based optimization methods in similar tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "For most SGD-based optimization tasks, starting with LR = 0.01, Momentum = 0.9 is a strong baseline.\n",
        "If instability is noticed, reduce LR slightly; if convergence is too slow, increase momentum slightly but avoid overshooting.\n",
        "\n",
        "---\n",
        "\n",
        "### ⏩ Conclusion\n",
        "\n",
        "| Learning Rate | Momentum | Performance                                  |\n",
        "|--------------|---------|----------------------------------------------- |\n",
        "| **0.01**     | **0.9**  | ✅ Best balance between speed and stability   |\n",
        "| **0.1**      | **0.9**  | ❌ Unstable, overshooting                     |\n",
        "| **0.001**    | **0.9**  | ⚠️ Too slow but stable                        |\n",
        "| **0.01**     | **0.95** | ❌ Overshooting                               |\n",
        "| **0.01**     | **0.8**  | ⚠️ Slower than 0.9 but stable                 |\n",
        "\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all the hyperparameter values tested. Your analysis should clearly illustrate how the choice of learning rate and momentum coefficient influences the performance of the Momentum optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QYRrs-VJN1c"
      },
      "source": [
        "# Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "The Nesterov Accelerated Gradient update is given by:\n",
        "\n",
        "$$\n",
        "v_t = \\beta \\, v_{t-1} + \\eta \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\beta \\, v_{t-1} - \\eta \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\( w_t \\\\) is the parameter vector at iteration \\\\( t \\\\),\n",
        "- \\\\( v_t \\\\) is the velocity (accumulated momentum) at iteration \\\\( t \\\\),\n",
        "- \\\\( \\eta \\\\) is the learning rate, and\n",
        "- \\\\( \\beta \\\\) is the momentum coefficient (typically a value between 0 and 1).\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Nesterov Accelerated Gradient (NAG) is a variant of SGD with momentum that computes the gradient at a \"lookahead\" position. Instead of computing the gradient at the current parameter \\\\( w_t \\\\) only, NAG uses the accumulated momentum from previous iterations to “look ahead” before updating the parameters. The update can be interpreted as:\n",
        "\n",
        "1. **Compute the momentum update:**  \n",
        "   \\\\( v_t = \\beta \\, v_{t-1} + \\eta \\, \\nabla f(w_t) \\\\)\n",
        "2. **Update the parameters using the previous momentum:**  \n",
        "   \\\\( w_{t+1} = w_t - \\beta \\, v_{t-1} - \\eta \\, \\nabla f(w_t) \\\\)\n",
        "\n",
        "This \"lookahead\" strategy often leads to faster convergence compared to standard momentum because it anticipates the effect of momentum and adjusts the update accordingly.\n",
        "\n",
        "*Note:* In our implementation, we assume that we cannot recompute the gradient at the lookahead position (for simplicity) so we approximate NAG with the following update:\n",
        "\n",
        "$$\n",
        "v_{\\text{prev}} = v_{t-1} \\quad,\\quad v_t = \\beta \\, v_{t-1} + \\eta \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\beta \\, v_{\\text{prev}} - \\eta \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "This formulation uses the previous momentum term directly for the \"lookahead\" component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WDQvhoyArOw4"
      },
      "outputs": [],
      "source": [
        "def nag_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    Nesterov Accelerated Gradient (NAG) update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to params.\n",
        "      - state: Dictionary to store the momentum (velocity). Initially empty.\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "            \"lr\": learning rate, and\n",
        "            \"momentum\": momentum coefficient.\n",
        "      - **kwargs: Additional arguments (ignored for NAG).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameter values.\n",
        "      - state: Updated state dictionary containing the velocity.\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]\n",
        "    momentum = hyperparams[\"momentum\"]\n",
        "\n",
        "    # Initialize the velocity if not already in state.\n",
        "    if \"v\" not in state:\n",
        "        state[\"v\"] = torch.zeros_like(params)\n",
        "\n",
        "    # Save the previous velocity for the lookahead.\n",
        "    v_prev = state[\"v\"].clone()\n",
        "    # Update the velocity.\n",
        "    state[\"v\"] = momentum * state[\"v\"] + lr * grad\n",
        "    # Update the parameters using the lookahead formula.\n",
        "    new_params = params - momentum * v_prev - lr * grad\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUmOpdw9GLU2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for Nesterov Accelerated Gradient.\n",
        "nag_hyperparams = {\n",
        "    \"lr\": 0.001,        # Learning rate\n",
        "    \"momentum\": 0.9     # Momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using NAG.\n",
        "run_all_optimizations(update_func=nag_update, hyperparams=nag_hyperparams, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Nesterov Accelerated Gradient.\n",
        "nag_hyperparams = {\n",
        "    \"lr\": 0.01,        # Learning rate\n",
        "    \"momentum\": 0.9     # Momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using NAG.\n",
        "run_all_optimizations(update_func=nag_update, hyperparams=nag_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eDjPlDb8-sdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Nesterov Accelerated Gradient.\n",
        "nag_hyperparams = {\n",
        "    \"lr\": 0.01,        # Learning rate\n",
        "    \"momentum\": 0.95     # Momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using NAG.\n",
        "run_all_optimizations(update_func=nag_update, hyperparams=nag_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5Y1Qy76f-slb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Nesterov Accelerated Gradient.\n",
        "nag_hyperparams = {\n",
        "    \"lr\": 0.001,        # Learning rate\n",
        "    \"momentum\": 0.95     # Momentum coefficient\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using NAG.\n",
        "run_all_optimizations(update_func=nag_update, hyperparams=nag_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dr-pLe3_-svR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNFNBxUSQvOe"
      },
      "source": [
        "# NAG Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the Nesterov Accelerated Gradient (NAG) optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    nag_hyperparams = {\n",
        "        \"lr\": 0.001,        # Learning rate\n",
        "        \"momentum\": 0.9     # Momentum coefficient\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various learning rates and momentum values to analyze how they affect the performance of NAG. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the learning rate and momentum values you tested. Provide a clear list or table of these values.\n",
        "\n",
        "### ✅ Answer\n",
        "| Learning Rate | Momentum |\n",
        "|--------------|----------|\n",
        "| 0.001        | 0.9      |\n",
        "| 0.01         | 0.9      |\n",
        "| 0.01         | 0.95     |\n",
        "| 0.001        | 0.95     |\n",
        "\n",
        "---\n",
        "2. **Performance Analysis:**\n",
        "   - How did varying the learning rate and momentum coefficient impact the convergence behavior of NAG?\n",
        "   - Were there any specific combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested parameter combinations cause the optimizer to get trapped in local minima?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "#### Impact on Convergence Behavior\n",
        "- Lower Learning Rate (0.001) with Momentum (0.9 & 0.95):\n",
        "  - Often resulted in local minima or slow convergence.\n",
        "  - Example: On the 1D Rastrigin and 2D Rastrigin functions, it found local minima near the initial point.\n",
        "  - On the 1D Ill-conditioned convex and 2D Rosenbrock functions, it successfully reached the global minimum, but likely at a slow pace.\n",
        "\n",
        "- Higher Learning Rate (0.01) with Momentum (0.9 & 0.95):\n",
        "  - More aggressive updates led to overshooting in some cases.\n",
        "  - Example: In the 2D Rosenbrock function, increasing the learning rate from 0.001 to 0.01 caused overshoot, meaning it oscillated around the optimal solution rather than converging smoothly.\n",
        "  - In the 1D Schwefel function, it showed a smoother learning curve at 0.01, momentum 0.9, but at 0.01, momentum 0.95, it got stuck near the initial point.\n",
        "\n",
        "- Unstable Behavior & Overshooting\n",
        "  - 2D Rosenbrock function with (lr=0.01, momentum=0.9 or 0.95)\n",
        "  - Caused overshooting, indicating that momentum was too high relative to the learning rate.\n",
        "  - 1D Schwefel function (lr=0.01, momentum=0.95)\n",
        "\n",
        "- Low learning rate (0.001) → slow convergence but stable results.\n",
        "- High learning rate (0.01) → potential overshooting.\n",
        "- Momentum 0.95 → risk of optimizer getting stuck or overshooting.\n",
        "- Momentum 0.9 was generally more stable but sometimes led to slow convergence.\n",
        "\n",
        "---\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of learning rate and momentum achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "For a good trade-off between speed and stability, use lr = 0.01, momentum = 0.9.\n",
        "\n",
        "The combination of **learning rate = 0.01 and momentum = 0.9** provides the best balance between convergence speed and stability because it accelerates learning without excessive overshooting or getting stuck in local minima. Unlike **lr = 0.001**, which often results in slow convergence and local minima trapping, **lr = 0.01** allows the optimizer to escape these traps while maintaining efficiency. Additionally, **momentum = 0.9** helps smooth updates and accelerate convergence without the instability observed at **momentum = 0.95**, which sometimes caused overshooting (e.g., in 2D Rosenbrock) or stagnation (e.g., in 1D Schwefel). This setting consistently led to smooth, decreasing learning curves across various functions, making it the most reliable choice.\n",
        "\n",
        "---\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying both the learning rate and momentum. For example, did higher momentum values speed up convergence but risk overshooting, or did lower momentum values result in more stable but slower convergence?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "- Higher learning rates helped escape local minima but needed controlled momentum to prevent overshooting.\n",
        "- Lower learning rates ensured stability but resulted in slow progress or local minima trapping.\n",
        "- Higher Momentum increased speed but risked instability.\n",
        "- Lower Momentum would likely slow down convergence further.\n",
        "---\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate and momentum when using NAG in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "For optimizing with Nesterov Accelerated Gradient (NAG), start with **lr = 0.01, momentum = 0.9**, as it offers a solid balance between speed and stability. If convergence is too slow, slightly increase **lr** or **momentum**, but be cautious of overshooting. For highly sensitive functions prone to instability, reduce **lr** while keeping **momentum at 0.9** to maintain steady progress. If loss curves oscillate, momentum is likely too high and should be lowered. Conversely, if the optimizer gets stuck in local minima, increasing momentum can help escape. Always monitor the loss curve trends and fine-tune parameters based on stability and convergence speed.\n",
        "\n",
        "---\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the NAG optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRT-aFgZKa8F"
      },
      "source": [
        "# Adagrad (Adaptive Gradient)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "Adagrad adapts the learning rate for each parameter individually by accumulating the squared gradients. Its update equations are:\n",
        "\n",
        "$$\n",
        "G_t = G_{t-1} + \\left(\\nabla f(w_t)\\right)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),  \n",
        "- \\\\(G_t\\\\) is the accumulation of squared gradients (applied elementwise),  \n",
        "- \\\\(\\eta\\\\) is the learning rate, and  \n",
        "- \\\\(\\epsilon\\\\) is a small constant added for numerical stability.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Adagrad adjusts the learning rate based on the history of gradients. Parameters with high gradients accumulate a larger \\\\(G_t\\\\) and thus receive a smaller effective learning rate, while parameters with small or sparse gradients receive larger updates. This makes Adagrad particularly useful for problems with sparse features, but note that its learning rate can decay too aggressively over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "laNX8UNNrOw5"
      },
      "outputs": [],
      "source": [
        "def adagrad_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    Adagrad update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to the parameters.\n",
        "      - state: Dictionary to store state (e.g., accumulated squared gradients).\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "          \"lr\": learning rate, and\n",
        "          \"eps\": a small constant for numerical stability.\n",
        "      - **kwargs: Additional arguments (ignored for Adagrad).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameters.\n",
        "      - state: Updated state dictionary.\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]\n",
        "    eps = hyperparams[\"eps\"]\n",
        "\n",
        "    if \"G\" not in state:\n",
        "        state[\"G\"] = torch.zeros_like(params)\n",
        "\n",
        "    # Accumulate squared gradients.\n",
        "    state[\"G\"] += grad**2\n",
        "\n",
        "    # Perform the Adagrad update.\n",
        "    new_params = params - lr * grad / (torch.sqrt(state[\"G\"]) + eps)\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lzeCmIFGwMN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for Adagrad.\n",
        "adagrad_hyperparams = {\n",
        "    \"lr\": 0.01,   # Learning rate\n",
        "    \"eps\": 1e-8   # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adagrad.\n",
        "run_all_optimizations(update_func=adagrad_update, hyperparams=adagrad_hyperparams, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adagrad.\n",
        "adagrad_hyperparams = {\n",
        "    \"lr\": 0.01,   # Learning rate\n",
        "    \"eps\": 1e-4   # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adagrad.\n",
        "run_all_optimizations(update_func=adagrad_update, hyperparams=adagrad_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4nxuaAXgHvNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adagrad.\n",
        "adagrad_hyperparams = {\n",
        "    \"lr\": 0.5,   # Learning rate\n",
        "    \"eps\": 1e-2   # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adagrad.\n",
        "run_all_optimizations(update_func=adagrad_update, hyperparams=adagrad_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J4JWRURxHvVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adagrad.\n",
        "adagrad_hyperparams = {\n",
        "    \"lr\": 0.5,   # Learning rate\n",
        "    \"eps\": 1e-4   # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adagrad.\n",
        "run_all_optimizations(update_func=adagrad_update, hyperparams=adagrad_hyperparams, epochs=100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-c1fTykPHvdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jz2gthSQ2cm"
      },
      "source": [
        "# Adagrad Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the Adagrad optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    adagrad_hyperparams = {\n",
        "        \"lr\": 0.01,   # Learning rate\n",
        "        \"eps\": 1e-8   # Small constant for numerical stability\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various learning rates and epsilon values to analyze how they affect the performance of Adagrad. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the learning rate and epsilon values you tested. Provide a clear list or table of these values.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "| Learning Rate | Epsilon |\n",
        "|--------------|----------|\n",
        "| 0.01         | 1e-8     |\n",
        "| 0.5          | 1e-8     |\n",
        "| 0.01         | 1e-4     |\n",
        "| 0.5          | 1e-4     |\n",
        "\n",
        "---\n",
        "2. **Performance Analysis:**\n",
        "   - How did different learning rates and epsilon values impact the convergence behavior of Adagrad?\n",
        "   - Were there any specific parameter combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested combinations cause the optimizer to get stuck in local minima?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Lower Learning Rate (0.01) → Slow Convergence and Local Minima Trapping\n",
        "- Higher Learning Rate (0.5) → Faster Convergence and Improved Escape from Local Minima\n",
        "- Increasing lr to 0.5 allowed the optimizer to escape local minima in 1D and 2D\n",
        "\n",
        "- Changing epsilon from 1e-8 to 1e-4 had minimal effect on convergence.\n",
        "- The primary driver of convergence behavior was the learning rate rather than epsilon.\n",
        "- Low lr in all cases resulted in very slow updates, often causing the optimizer to barely move.\n",
        "- High lr improved convergence but did not always reach a precise global minimum.\n",
        "\n",
        "---\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of learning rate and epsilon achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Best practice: Learning Rate = 0.5, Epsilon = 1e-8 or 1e-4\n",
        "\n",
        "- **Faster Convergence Without Getting Stuck**: Unlike lr = 0.01, which often resulted in almost no movement, lr = 0.5 allowed the optimizer to escape local minima and make meaningful progress across functions like 1D Rastrigin, 2D Rastrigin, and 2D Rosenbrock.\n",
        "- **Stable Updates Without Overshooting**: Unlike momentum-based optimizers, Adagrad naturally adapts the learning rate, preventing excessive oscillations.\n",
        "- **Better Exploration of Complex Functions**: The optimizer successfully reduced loss in challenging landscapes like 2D Rosenbrock and Ill-conditioned convex, where lower learning rates had trouble escaping local traps.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying the learning rate and epsilon values. For instance, did a lower epsilon improve stability at the cost of slower convergence?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "- Low learning rate (0.01) consistently caused slow convergence and local minima trapping.\n",
        "- Higher learning rate (0.5) enabled fast convergence, but final solutions were sometimes near-optimal rather than exact.\n",
        "- Changing epsilon had negligible effects compared to adjusting the learning rate.\n",
        "- Adagrad’s adaptive nature prevented overshooting, so tuning learning rate was more important than tuning epsilon.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate and epsilon when using Adagrad in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "For most optimization tasks, set lr = 0.5 and epsilon = 1e-8 as a strong default. Adjust the learning rate only if convergence is too slow (increase lr) or unstable (slightly decrease lr). There is usually no need to modify epsilon, as it has little impact on Adagrad’s overall performance.\n",
        "\n",
        "---\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Adagrad optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ZHeL0ELAkY"
      },
      "source": [
        "# RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "RMSprop adapts the learning rate for each parameter by maintaining an exponentially decaying average of squared gradients. The update equations are:\n",
        "\n",
        "$$\n",
        "E[g^2]_t = \\beta \\, E[g^2]_{t-1} + (1 - \\beta) \\, \\left(\\nabla f(w_t)\\right)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
        "- \\\\(E[g^2]_t\\\\) is the exponentially decaying average of squared gradients,\n",
        "- \\\\(\\eta\\\\) is the learning rate,\n",
        "- \\\\(\\beta\\\\) is the decay rate (typically around 0.9), and\n",
        "- \\\\(\\epsilon\\\\) is a small constant added for numerical stability.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "RMSprop is designed to overcome the aggressive decay of the learning rate seen in Adagrad by using an exponential decay factor \\\\(\\beta\\\\) to forget old gradients. This helps to maintain a more stable and adaptive learning rate during training. RMSprop works well in practice on non-stationary problems and deep networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2USzXZaeG5x8"
      },
      "outputs": [],
      "source": [
        "def rmsprop_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    RMSprop update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to parameters.\n",
        "      - state: Dictionary to store state (e.g., running average of squared gradients).\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "            \"lr\": learning rate,\n",
        "            \"beta\": decay rate, and\n",
        "            \"eps\": a small constant for numerical stability.\n",
        "      - **kwargs: Additional arguments (ignored for RMSprop).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameters.\n",
        "      - state: Updated state dictionary.\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]\n",
        "    beta = hyperparams[\"beta\"]\n",
        "    eps = hyperparams[\"eps\"]\n",
        "\n",
        "    if \"E\" not in state:\n",
        "        state[\"E\"] = torch.zeros_like(params)\n",
        "\n",
        "    # Update the running average of squared gradients.\n",
        "    state[\"E\"] = beta * state[\"E\"] + (1 - beta) * grad**2\n",
        "\n",
        "    # Compute the RMSprop update.\n",
        "    new_params = params - lr * grad / (torch.sqrt(state[\"E\"]) + eps)\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG1FUJg7G7AS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for RMSprop.\n",
        "rmsprop_hyperparams = {\n",
        "    \"lr\": 0.001,    # Learning rate\n",
        "    \"beta\": 0.9,    # Decay rate\n",
        "    \"eps\": 1e-3     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using RMSprop.\n",
        "run_all_optimizations(update_func=rmsprop_update, hyperparams=rmsprop_hyperparams, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for RMSprop.\n",
        "rmsprop_hyperparams = {\n",
        "    \"lr\": 0.1,    # Learning rate\n",
        "    \"beta\": 0.9,    # Decay rate\n",
        "    \"eps\": 1e-8     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using RMSprop.\n",
        "run_all_optimizations(update_func=rmsprop_update, hyperparams=rmsprop_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6u1Y0iUXPwI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for RMSprop.\n",
        "rmsprop_hyperparams = {\n",
        "    \"lr\": 0.001,    # Learning rate\n",
        "    \"beta\": 0.95,    # Decay rate\n",
        "    \"eps\": 1e-8     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using RMSprop.\n",
        "run_all_optimizations(update_func=rmsprop_update, hyperparams=rmsprop_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "fxi4cA6NPwKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for RMSprop.\n",
        "rmsprop_hyperparams = {\n",
        "    \"lr\": 0.1,    # Learning rate\n",
        "    \"beta\": 0.95,    # Decay rate\n",
        "    \"eps\": 1e-8     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using RMSprop.\n",
        "run_all_optimizations(update_func=rmsprop_update, hyperparams=rmsprop_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "XArvsj9-PwYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for RMSprop.\n",
        "rmsprop_hyperparams = {\n",
        "    \"lr\": 0.001,    # Learning rate\n",
        "    \"beta\": 0.9,    # Decay rate\n",
        "    \"eps\": 1e-8     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using RMSprop.\n",
        "run_all_optimizations(update_func=rmsprop_update, hyperparams=rmsprop_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "MqkqNM1LU68t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSprop Hyperparameter Analysis (5 Points)\n",
        "\n",
        "## 1. Parameter Variation:\n",
        "| Learning Rate | Beta  | Epsilon |\n",
        "|--------------|-------|----------|\n",
        "| 0.001        | 0.9   | 1e-3     |\n",
        "| 0.1          | 0.9   | 1e-8     |\n",
        "| 0.001        | 0.95  | 1e-8     |\n",
        "| 0.1          | 0.95  | 1e-8     |\n",
        "| 0.001        | 0.9   | 1e-8     |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 2. Performance Analysis:\n",
        "\n",
        "- Effect of Learning Rate (lr):\n",
        "  - Lower Learning Rate (0.001): Provided stability but resulted in very slow convergence.\n",
        "  - Struggled to escape local minima, especially in complex loss landscapes.\n",
        "  - More suitable for highly sensitive tasks requiring careful, gradual updates.\n",
        "\n",
        "- Higher Learning Rate (0.1):\n",
        "\n",
        "  - Faster convergence across most test cases, significantly outperforming lr = 0.001.\n",
        "  - Showed occasional instability and oscillations, especially when paired with lower beta values.\n",
        "  - Worked best when combined with β = 0.95, which smoothed out excessive fluctuations.\n",
        "\n",
        "- Effect of Beta:\n",
        " - Lower Beta (0.9): Allowed for faster adaptation to recent gradients but could introduce more noise.\n",
        "   - Led to marginally better responsiveness, making updates slightly quicker.\n",
        "   - In some cases, introduced small oscillations when paired with a high learning rate.\n",
        "\n",
        " - Higher Beta (0.95):\n",
        "   - Retained historical gradient information for a longer period, smoothing out updates.\n",
        "   - Helped reduce instability when using a high learning rate (0.1), leading to better convergence.\n",
        "   - Slightly slower updates compared to β = 0.9, but this trade-off helped avoid erratic jumps.\n",
        "\n",
        "- Effect of Epsilon:\n",
        "  - Higher Epsilon (1e-3): Improved numerical stability but slightly slowed down learning.\n",
        "  - Best suited for situations where gradients are extremely small or erratic.\n",
        "\n",
        "  - Lower Epsilon (1e-8): Allowed for more aggressive updates, making learning more responsive.\n",
        "  - More effective when paired with a higher learning rate (0.1).\n",
        "  - Risked slight instability in certain cases but was generally preferred for smoother optimizations.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Optimal Parameter Combination:\n",
        "\n",
        "✅ Best Settings: lr = 0.1, β = 0.95, ε = 1e-8\n",
        "\n",
        "🔹 Why This is Optimal:\n",
        "- Achieved the best balance between convergence speed and stability.\n",
        "- Higher β (0.95) controlled oscillations, preventing overshooting while maintaining momentum.\n",
        "- Lower ε (1e-8) allowed efficient learning, without unnecessary dampening.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Trade-offs and Observations:\n",
        "\n",
        "| Hyperparameter | Effect |\n",
        "|---------------|--------|\n",
        "| **Low lr (0.001)** | Stable but very slow, struggles to escape local minima. |\n",
        "| **High lr (0.1)** | Fast convergence but needs β = 0.95 for stability. |\n",
        "| **Low β (0.9)** | Faster adaptation but slightly noisier updates. |\n",
        "| **High β (0.95)** | Smoother updates, more stable but slightly slower. |\n",
        "| **Low ε (1e-8)** | Works best for learning efficiency, may introduce slight instability. |\n",
        "| **High ε (1e-3)** | Ensures stability, but slows down convergence. |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Recommendations:\n",
        "\n",
        "- Start with lr = 0.1, β = 0.95, and ε = 1e-8 for a strong balance of speed and stability.\n",
        "- Lower lr to 0.001 if dealing with highly sensitive gradients or extreme noise.\n",
        "- Use β = 0.9 for slightly faster updates, but prefer β = 0.95 for smoother learning.\n",
        "- Keep ε at 1e-8 unless encountering numerical instability, in which case 1e-3 can be used."
      ],
      "metadata": {
        "id": "uPcUEDEESnw9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx1B75YsHD5-"
      },
      "source": [
        "# Adam (Adaptive Moment Estimation)(10 Points)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "Adam computes two exponential moving averages of the gradients:\n",
        "\n",
        "- **First moment (mean):**\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "- **Second moment (uncentered variance):**\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\left(\\nabla f(w_t)\\right)^2\n",
        "$$\n",
        "\n",
        "Bias corrections are then applied:\n",
        "\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
        "$$\n",
        "\n",
        "Finally, the parameter update is given by:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\, \\hat{m}_t\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
        "- \\\\(\\eta\\\\) is the learning rate,\n",
        "- \\\\(\\beta_1\\\\) and \\\\(\\beta_2\\\\) are the exponential decay rates for the moment estimates, and\n",
        "- \\\\(\\epsilon\\\\) is a small constant to avoid division by zero.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Adam combines ideas from momentum and RMSprop. By maintaining an exponentially decaying average of past gradients (first moment) and past squared gradients (second moment) along with bias correction, Adam adapts the learning rate for each parameter. This makes it especially suitable for problems with noisy or sparse gradients, as it works well \"out of the box\" with minimal tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNV6AQIzRFlK"
      },
      "source": [
        "# RMSprop Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the RMSprop optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    rmsprop_hyperparams = {\n",
        "        \"lr\": 0.001,    # Learning rate\n",
        "        \"beta\": 0.9,    # Decay rate\n",
        "        \"eps\": 1e-8     # Small constant for numerical stability\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various learning rates, beta values, and epsilon values to analyze how they affect the performance of RMSprop. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the learning rate, beta, and epsilon values you tested. Provide a clear list or table of these values.\n",
        "\n",
        "2. **Performance Analysis:**\n",
        "   - How did different combinations of learning rate, beta, and epsilon impact the convergence behavior of RMSprop?\n",
        "   - Were there any specific combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested combinations cause the optimizer to get stuck in local minima?\n",
        "\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of parameters achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying these hyperparameters. For example, did a higher beta improve stability at the cost of slower adaptation?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate, beta, and epsilon when using RMSprop in similar optimization tasks?\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the RMSprop optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_1ygVwm-G8Gc"
      },
      "outputs": [],
      "source": [
        "def adam_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    Adam update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to params.\n",
        "      - state: Dictionary to store moving averages for first and second moments.\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "            \"lr\": learning rate,\n",
        "            \"beta1\": decay rate for the first moment,\n",
        "            \"beta2\": decay rate for the second moment,\n",
        "            \"eps\": a small constant for numerical stability.\n",
        "      - **kwargs: Additional arguments (ignored for Adam).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameter values.\n",
        "      - state: Updated state dictionary.\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]\n",
        "    beta1 = hyperparams[\"beta1\"]\n",
        "    beta2 = hyperparams[\"beta2\"]\n",
        "    eps = hyperparams[\"eps\"]\n",
        "\n",
        "    if \"m\" not in state:\n",
        "        # TODO: Initialize first moment (m), second moment (v), and timestep (t)\n",
        "        state[\"m\"] = torch.zeros_like(params)\n",
        "        state[\"v\"] = torch.zeros_like(params)\n",
        "        state[\"t\"] = 0\n",
        "\n",
        "    # TODO: Update timestep\n",
        "    state[\"t\"] += 1\n",
        "\n",
        "    # TODO: Compute biased first moment estimate (m) and second moment estimate (v)\n",
        "    state[\"m\"] = beta1*state[\"m\"] + (1 - beta1)*grad\n",
        "    state[\"v\"] = beta2*state[\"v\"] + (1 - beta2)*(grad**2)\n",
        "\n",
        "    # TODO: Compute bias-corrected first moment estimate (m_hat) and second moment estimate (v_hat)\n",
        "    m_hat = state[\"m\"] / (1 - beta1**state[\"t\"])\n",
        "    v_hat = state[\"v\"] / (1 - beta2**state[\"t\"])\n",
        "\n",
        "    # TODO: Update parameters using Adam's rule\n",
        "    new_params = params - lr*(m_hat / (torch.sqrt(v_hat) + eps))\n",
        "\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLTL4r2tHFj9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for Adam.\n",
        "adam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Smoothing constant\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adam.\n",
        "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adam.\n",
        "adam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.85,     # Decay rate for first moment\n",
        "    \"beta2\": 0.98,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Smoothing constant\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adam.\n",
        "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "bD-V2S5RVxIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adam.\n",
        "adam_hyperparams = {\n",
        "    \"lr\": 0.01,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Smoothing constant\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adam.\n",
        "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "Nq8uH_TdVxK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adam.\n",
        "adam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.85,     # Decay rate for first moment\n",
        "    \"beta2\": 0.98,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Smoothing constant\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adam.\n",
        "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "el0rObjyVxNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adam.\n",
        "adam_hyperparams = {\n",
        "    \"lr\": 0.1,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Smoothing constant\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adam.\n",
        "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "rgp0xmRUWYkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Adam.\n",
        "adam_hyperparams = {\n",
        "    \"lr\": 0.1,      # Learning rate\n",
        "    \"beta1\": 0.85,     # Decay rate for first moment\n",
        "    \"beta2\": 0.98,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Smoothing constant\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Adam.\n",
        "run_all_optimizations(update_func=adam_update, hyperparams=adam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "fApJZiijWd_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBOnUPf5RTpR"
      },
      "source": [
        "# Adam Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the Adam optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    adam_hyperparams = {\n",
        "        \"lr\": 0.001,      # Learning rate\n",
        "        \"beta1\": 0.9,     # Decay rate for first moment\n",
        "        \"beta2\": 0.999,   # Decay rate for second moment\n",
        "        \"eps\": 1e-8       # Smoothing constant\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various combinations of the learning rate, beta1, beta2, and epsilon values to analyze how they affect the performance of Adam. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the values you tested for learning rate, beta1, beta2, and epsilon. Provide a clear list or table of these parameter combinations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "| Learning Rate (lr) | Beta1 (β₁) | Beta2 (β₂) |\n",
        "|-------------------|------------|------------|\n",
        "| **0.001**        | **0.9**     | **0.999**  |\n",
        "| **0.001**        | **0.85**    | **0.98**   |\n",
        "| **0.01**         | **0.9**     | **0.999**  |\n",
        "| **0.01**         | **0.85**    | **0.98**   |\n",
        "| **0.1**          | **0.9**     | **0.999**  |\n",
        "| **0.1**          | **0.85**    | **0.98**   |\n",
        "\n",
        "2. **Performance Analysis:**\n",
        "   - How did different parameter combinations impact the convergence behavior of Adam?\n",
        "   - Were there any specific combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested parameter combinations cause the optimizer to get stuck in local minima?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Effect of Learning Rate:\n",
        "  - Lower LR (0.001): Very stable, but optimization was slow and sometimes got stuck in local minima. Performed better with lower decay rates (β₁ = 0.85, β₂ = 0.98) due to increased adaptation.\n",
        "\n",
        " - Moderate LR (0.01): Balanced speed and stability, leading to better convergence. Best results when paired with β₁ = 0.9 and β₂ = 0.999, as it maintained sufficient momentum while adapting quickly.\n",
        "\n",
        "  - Higher LR (0.1): Fastest convergence, but increased risk of instability and oscillations, especially with β₁ = 0.85.\n",
        "\n",
        "- Effect of Beta1:\n",
        " - Lower β₁ (0.85): Allowed faster adaptation to new gradients, improving performance in dynamic loss landscapes. Increased sensitivity to noise, making updates more aggressive.\n",
        "\n",
        "\n",
        " - Higher β₁ (0.9): Retained past gradient information longer, resulting in smoother updates. Preferred for higher learning rates (0.1) as it reduced instability.\n",
        "\n",
        "- Effect of Beta2:\n",
        "  - Lower β₂ (0.98): Allowed quicker adaptation to gradient updates, improving responsiveness. Performed better at lower learning rates (0.001, 0.01) but introduced oscillations at 0.1 LR.\n",
        "\n",
        " - Higher β₂ (0.999): Provided greater stability by averaging squared gradients over a longer window. Slowed down adaptation, sometimes preventing rapid convergence.\n",
        "Best suited for higher learning rates (0.1) to counteract instability.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of parameters achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Best Settings: lr = 0.01, β₁ = 0.9, β₂ = 0.999\n",
        "\n",
        " - Balanced stability and adaptability, leading to consistent convergence.\n",
        " - Lower β₂ (0.98) allowed faster updates, but β₂ = 0.999 ensured smoother learning.\n",
        " - Avoided instability that occurred at lr = 0.1 while still being faster than lr = 0.001.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying these hyperparameters. For example, did a lower epsilon or higher beta values improve stability at the cost of slower convergence?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### Answer\n",
        "\n",
        "| Hyperparameter | Effect |\n",
        "|---------------|--------|\n",
        "| **Low lr (0.001)** | Very stable but slow; struggles to escape local minima. |\n",
        "| **Moderate lr (0.01)** | Best balance of speed and stability. |\n",
        "| **High lr (0.1)** | Fast convergence but risks instability without high β values. |\n",
        "| **Low β₁ (0.85)** | Faster adaptation but noisier updates. |\n",
        "| **High β₁ (0.9)** | Smoother updates, reducing instability at high LR. |\n",
        "| **Low β₂ (0.98)** | Faster adaptation but risked oscillations. |\n",
        "| **High β₂ (0.999)** | More stable but slightly slower optimization. |\n",
        "\n",
        "---\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate, beta1, beta2, and epsilon when using Adam in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Start with lr = 0.01, β₁ = 0.9, β₂ = 0.999 for a strong balance of convergence speed and stability.\n",
        "- If training is slow, reduce β₂ to 0.98 for quicker adaptation.\n",
        "- If instability occurs, increase β₂ to 0.999 and reduce lr slightly.\n",
        "- For highly noisy gradients, prefer β₁ = 0.9 to smooth out updates.\n",
        "---\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Adam optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y7c6J7rHi1Q"
      },
      "source": [
        "# Newton’s Method(5 Points)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "Newton’s method uses second-order information (the Hessian) to update the parameters. The update rule is given by:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - H(w_t)^{-1} \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\( w_t \\\\) is the parameter vector at iteration \\\\( t \\\\),\n",
        "- \\\\( \\nabla f(w_t) \\\\) is the gradient of the loss function at \\\\( w_t \\\\), and\n",
        "- \\\\( H(w_t) \\\\) is the Hessian matrix (i.e., the matrix of second derivatives) evaluated at \\\\( w_t \\\\).\n",
        "\n",
        "For numerical stability (and to handle cases when the Hessian is nearly singular), a small constant \\\\( \\epsilon \\\\) is added to the diagonal of the Hessian:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\left( H(w_t) + \\epsilon I \\right)^{-1} \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Newton’s method can converge very rapidly (quadratically) when the loss function is well-behaved and when you are close to the optimum. However, computing the full Hessian (and its inverse) can be computationally expensive for high-dimensional problems. Because of these challenges, Newton’s method (or its quasi-Newton variants like L-BFGS) is typically used for smaller models or as part of a hybrid optimization strategy.\n",
        "\n",
        "*Note:* In our implementation, we assume the loss function is defined using PyTorch, the parameters are a torch tensor of shape \\\\( (n,) \\\\) for 1D functions or \\\\( (2,) \\\\) for 2D functions, and the Hessian is computed via PyTorch’s automatic differentiation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "y4p-AIjsHG32"
      },
      "outputs": [],
      "source": [
        "def newton_update(params, grad, state, hyperparams, hessian):\n",
        "    \"\"\"\n",
        "    Newton's Method update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor, shape (n,) for 1D or (2,) for 2D).\n",
        "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
        "      - state: Dictionary to store state (not used in basic Newton's method).\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "            \"eps\": A small constant for numerical stability.\n",
        "      - hessian: Hessian matrix of the loss function (torch tensor of shape (n, n)).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameter values.\n",
        "      - state: Unchanged state dictionary.\n",
        "    \"\"\"\n",
        "    eps = hyperparams[\"eps\"]\n",
        "\n",
        "    # TODO: Create an identity matrix of the same size as params\n",
        "    I = torch.eye(params.shape[0])\n",
        "\n",
        "    # TODO: Regularize the Hessian for numerical stability\n",
        "    H_reg = hessian + eps * I\n",
        "\n",
        "    # TODO: Compute the inverse of the regularized Hessian\n",
        "    H_inv = torch.inverse(H_reg)\n",
        "\n",
        "    # TODO: Compute the Newton update step: new_params = params - H_inv @ grad\n",
        "    new_params = params - (H_inv) @ grad\n",
        "\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRi7NpomHnW7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for Newton's method.\n",
        "newton_hyperparams = {\n",
        "    \"eps\": 1e-4   # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Newton's method.\n",
        "# Note: Newton's method is a second-order method, so our general optimize function should be called with second_order=True.\n",
        "run_all_optimizations(update_func=newton_update, hyperparams=newton_hyperparams, epochs=100, second_order=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRoCPZNuRqWb"
      },
      "source": [
        "# Newton's Method Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For Newton's Method, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    newton_hyperparams = {\n",
        "        \"eps\": 1e-4   # Small constant for numerical stability\n",
        "    }\n",
        "\n",
        "In this section, you are required to test different values for the epsilon parameter to analyze its effect on the performance of Newton's Method. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the epsilon values you tested. Provide a clear list or table of these values.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "Newton’s method was tested with an epsilon value of 1e-4, chosen to ensure numerical stability while maintaining effective convergence.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Performance Analysis:**\n",
        "   - How did different epsilon values impact the convergence behavior of Newton's Method?\n",
        "   - Were there any specific epsilon values that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested epsilon values cause the optimizer to get stuck in local minima or result in numerical instability?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "Newton’s method demonstrated rapid convergence on well-behaved functions, particularly those with smooth, convex surfaces. It performed exceptionally well on functions Ill-Conditioned, Rosenbrock, and 2D Ill-Conditioned, where second-order updates efficiently guided optimization. However, the method exhibited overshooting or converging along suboptimal paths in several tests due to its aggressive step size. Additionally, it frequently became trapped in local minima.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Optimal Parameter:**\n",
        "   - Which epsilon value, among those tested, achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider this epsilon value optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "The optimal choice of epsilon (ε = 1e-4) provided sufficient numerical stability, preventing division errors in Hessian inversion while ensuring effective optimization. This small value helped maintain convergence reliability without significantly impairing the step size.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying epsilon. For example, did a smaller epsilon provide more precise curvature estimates at the cost of slower convergence?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "Newton’s method offers extremely fast and accurate convergence near global minima, making it ideal for well-conditioned or convex problems. However, its reliance on second-order derivatives makes it highly sensitive to initial values and computationally expensive due to the need for Hessian computation and inversion. While it efficiently optimizes smooth landscapes, it struggles with non-convex or ill-conditioned problems, where it frequently gets stuck in local minima or takes excessively large steps, leading to instability.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the epsilon parameter when using Newton's Method in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "Newton’s method excels in optimizing well-conditioned, convex, and low-dimensional problems, where leveraging second-order information significantly enhances performance. Keeping epsilon small (1e-4) helps maintain numerical stability while preserving an effective convergence rate.\n",
        "\n",
        "---\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all epsilon values tested. Your analysis should clearly illustrate how the choice of this hyperparameter influences the performance of Newton's Method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyaCL1hSKpec"
      },
      "source": [
        "# L-BFGS (Limited-memory BFGS)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "L-BFGS is a quasi-Newton method that approximates the inverse Hessian matrix using a limited history of parameter and gradient differences. It avoids storing the full Hessian, making it memory efficient for large-scale problems.\n",
        "\n",
        "Suppose at iteration \\\\(t\\\\) we have stored \\\\(m\\\\) pairs \\\\((s_i, y_i)\\\\) for \\\\(i = t-m, \\ldots, t-1\\\\), where\n",
        "\n",
        "$$\n",
        "s_i = w_{i+1} - w_i \\quad \\text{and} \\quad y_i = \\nabla f(w_{i+1}) - \\nabla f(w_i).\n",
        "$$\n",
        "\n",
        "The two-loop recursion for computing the approximate inverse Hessian times the gradient \\\\(q = \\nabla f(w_t)\\\\) is as follows:\n",
        "\n",
        "1. **First Loop (backward pass):**\n",
        "\n",
        "   For \\\\(i = t-1, t-2, \\ldots, t-m\\\\):\n",
        "\n",
        "   $$\n",
        "   \\rho_i = \\frac{1}{y_i^T s_i} \\quad,\\quad \\alpha_i = \\rho_i \\, s_i^T q\n",
        "   $$\n",
        "\n",
        "   Update:\n",
        "\n",
        "   $$\n",
        "   q \\leftarrow q - \\alpha_i \\, y_i\n",
        "   $$\n",
        "\n",
        "2. **Scaling the Initial Hessian:**\n",
        "\n",
        "   Use an initial Hessian approximation:\n",
        "\n",
        "   $$\n",
        "   H_0 = \\gamma I \\quad \\text{with} \\quad \\gamma = \\frac{s_{t-1}^T y_{t-1}}{y_{t-1}^T y_{t-1}}\n",
        "   $$\n",
        "\n",
        "   Set:\n",
        "\n",
        "   $$\n",
        "   r = H_0 q\n",
        "   $$\n",
        "\n",
        "3. **Second Loop (forward pass):**\n",
        "\n",
        "   For \\\\(i = t-m, \\ldots, t-1\\\\):\n",
        "\n",
        "   $$\n",
        "   \\beta_i = \\rho_i \\, y_i^T r\n",
        "   $$\n",
        "\n",
        "   Update:\n",
        "\n",
        "   $$\n",
        "   r \\leftarrow r + s_i (\\alpha_i - \\beta_i)\n",
        "   $$\n",
        "\n",
        "The parameter update is then:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - r.\n",
        "$$\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "L-BFGS efficiently approximates the Newton direction without computing or storing the full Hessian matrix. By keeping a limited memory of recent changes in parameters and gradients, it builds a useful approximation of the inverse Hessian that can lead to fast convergence on large-scale problems. The hyperparameter \\\\(m\\\\) controls the memory size (i.e., how many past updates are retained), and the initial scaling factor \\\\(\\gamma\\\\) helps initialize the inverse Hessian approximation.\n",
        "\n",
        "*Note:* In our implementation, we maintain lists of differences \\\\(s_i\\\\) and \\\\(y_i\\\\), and use the two-loop recursion to compute the search direction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ObUW2ZHjH0R1"
      },
      "outputs": [],
      "source": [
        "def lbfgs_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    L-BFGS update function (simplified version).\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
        "      - state: Dictionary to store state.\n",
        "          Expected keys:\n",
        "            \"S\": list of previous parameter differences.\n",
        "            \"Y\": list of previous gradient differences.\n",
        "            \"prev_params\": previous parameters (torch tensor).\n",
        "            \"prev_grad\": previous gradient (torch tensor).\n",
        "      - hyperparams: Dictionary of hyperparameters.\n",
        "          Must contain:\n",
        "            \"lr\": initial learning rate scaling factor,\n",
        "            \"m\": memory size (maximum number of corrections to store).\n",
        "      - **kwargs: Additional arguments (ignored for L-BFGS).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameter values.\n",
        "      - state: Updated state dictionary.\n",
        "    \"\"\"\n",
        "    lr = hyperparams[\"lr\"]\n",
        "    m_memory = hyperparams[\"m\"]\n",
        "\n",
        "    # If no previous state exists, do a simple gradient descent step.\n",
        "    if \"prev_params\" not in state:\n",
        "        new_params = params - lr * grad\n",
        "        state[\"prev_params\"] = params.clone().detach()\n",
        "        state[\"prev_grad\"] = grad.clone().detach()\n",
        "        state[\"S\"] = []\n",
        "        state[\"Y\"] = []\n",
        "        return new_params, state\n",
        "\n",
        "    # Compute parameter difference s and gradient difference y.\n",
        "    s = params - state[\"prev_params\"]\n",
        "    y = grad - state[\"prev_grad\"]\n",
        "\n",
        "    # Update history lists.\n",
        "    if len(state[\"S\"]) >= m_memory:\n",
        "        state[\"S\"].pop(0)\n",
        "        state[\"Y\"].pop(0)\n",
        "    state[\"S\"].append(s.clone().detach())\n",
        "    state[\"Y\"].append(y.clone().detach())\n",
        "\n",
        "    # Update previous parameters and gradient.\n",
        "    state[\"prev_params\"] = params.clone().detach()\n",
        "    state[\"prev_grad\"] = grad.clone().detach()\n",
        "\n",
        "    # Two-loop recursion.\n",
        "    q = grad.clone().detach()\n",
        "    alpha = []\n",
        "    # Loop from the most recent history to the oldest.\n",
        "    for s_i, y_i in zip(reversed(state[\"S\"]), reversed(state[\"Y\"])):\n",
        "        rho_i = 1.0 / (torch.dot(y_i.view(-1), s_i.view(-1)) + 1e-10)\n",
        "        a_i = rho_i * torch.dot(s_i.view(-1), q.view(-1))\n",
        "        alpha.append(a_i)\n",
        "        q = q - a_i * y_i\n",
        "\n",
        "    # Scaling for initial Hessian approximation.\n",
        "    if len(state[\"Y\"]) > 0:\n",
        "        last_y = state[\"Y\"][-1]\n",
        "        last_s = state[\"S\"][-1]\n",
        "        gamma = torch.dot(last_y.view(-1), last_s.view(-1)) / (torch.dot(last_y.view(-1), last_y.view(-1)) + 1e-10)\n",
        "    else:\n",
        "        gamma = 1.0\n",
        "\n",
        "    r = gamma * q\n",
        "    # Forward loop: from oldest to newest.\n",
        "    for s_i, y_i, a_i in zip(state[\"S\"], state[\"Y\"], reversed(alpha)):\n",
        "        rho_i = 1.0 / (torch.dot(y_i.view(-1), s_i.view(-1)) + 1e-10)\n",
        "        beta = rho_i * torch.dot(y_i.view(-1), r.view(-1))\n",
        "        r = r + s_i * (a_i - beta)\n",
        "\n",
        "    # Update parameters: move in the direction r.\n",
        "    new_params = params - r\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No4LuEnjKsOY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for L-BFGS.\n",
        "lbfgs_hyperparams = {\n",
        "    \"lr\": 0.01,   # initial learning rate scaling factor\n",
        "    \"m\": 10       # memory size (maximum corrections to store)\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using L-BFGS.\n",
        "run_all_optimizations(update_func=lbfgs_update, hyperparams=lbfgs_hyperparams, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for L-BFGS.\n",
        "lbfgs_hyperparams = {\n",
        "    \"lr\": 0.01,   # initial learning rate scaling factor\n",
        "    \"m\": 15       # memory size (maximum corrections to store)\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using L-BFGS.\n",
        "run_all_optimizations(update_func=lbfgs_update, hyperparams=lbfgs_hyperparams, epochs=100)"
      ],
      "metadata": {
        "id": "ERB6flNsdMke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for L-BFGS.\n",
        "lbfgs_hyperparams = {\n",
        "    \"lr\": 0.001,   # initial learning rate scaling factor\n",
        "    \"m\": 10       # memory size (maximum corrections to store)\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using L-BFGS.\n",
        "run_all_optimizations(update_func=lbfgs_update, hyperparams=lbfgs_hyperparams, epochs=100)"
      ],
      "metadata": {
        "id": "fr2161ZRdMs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for L-BFGS.\n",
        "lbfgs_hyperparams = {\n",
        "    \"lr\": 0.001,   # initial learning rate scaling factor\n",
        "    \"m\": 15       # memory size (maximum corrections to store)\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using L-BFGS.\n",
        "run_all_optimizations(update_func=lbfgs_update, hyperparams=lbfgs_hyperparams, epochs=100)"
      ],
      "metadata": {
        "id": "moViObBKdM0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1iVUH2tR0Rc"
      },
      "source": [
        "# L-BFGS Hyperparameter Analysis\n",
        "\n",
        "For the L-BFGS optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    lbfgs_hyperparams = {\n",
        "        \"lr\": 0.01,   # initial learning rate scaling factor\n",
        "        \"m\": 10       # memory size (maximum corrections to store)\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various values for the learning rate and memory size to analyze how they affect the performance of L-BFGS. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the learning rate and memory size (m) values you tested. Provide a clear list or table of these parameter combinations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "| Learning Rate (LR) | Memory Size (m) |\n",
        "|--------------------|-----------------|\n",
        "| 0.001             | 15              |\n",
        "| 0.001             | 10              |\n",
        "| 0.01              | 15              |\n",
        "| 0.01              | 10              |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. **Performance Analysis:**\n",
        "   - How did different combinations of learning rate and memory size impact the convergence behavior of L-BFGS?\n",
        "   - Were there any specific parameter combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested combinations cause the optimizer to get trapped in local minima or exhibit other issues?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Effect of Learning Rate (LR):\n",
        "  - A lower LR (0.001) resulted in stable but very slow convergence, often struggling to escape local minima.\n",
        "  - A higher LR (0.01) accelerated convergence, but in some cases led to oscillations or minor instability.\n",
        "- Effect of Memory Size (m):\n",
        "  - Larger memory (15) provided slightly improved Hessian approximations, leading to more stable convergence in some cases.\n",
        "  - Smaller memory (10) still performed well, but occasionally led to suboptimal updates, particularly when combined with a higher learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of learning rate and memory size achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- LR = 0.01, Memory Size = 15\n",
        "\n",
        "This combination achieved the best balance of convergence speed and stability, allowing efficient optimization without excessive oscillations.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying these hyperparameters. For example, did a higher memory size improve convergence at the cost of increased computation time?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Low LR (0.001): Stable but extremely slow, struggling with local minima.\n",
        "- High LR (0.01): Faster, but required careful tuning to prevent instability.\n",
        "- Low Memory (10): Worked well but slightly weaker Hessian approximations.\n",
        "- High Memory (15): Slightly improved updates, but gains were not always significant.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate and memory size when using L-BFGS in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Use LR = 0.01 for faster convergence but monitor for instability.\n",
        "- Prefer Memory Size = 15 for better Hessian approximations.\n",
        "-Avoid excessively low learning rates (0.001) unless stability is a top priority.\n",
        "\n",
        "---\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the L-BFGS optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYPm-EFdK1YX"
      },
      "source": [
        "# Nadam (Nesterov-accelerated Adam)\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "Nadam combines the ideas of Adam and Nesterov momentum. Its update equations are as follows:\n",
        "\n",
        "1. **First Moment Update:**\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "2. **Second Moment Update:**\n",
        "\n",
        "$$\n",
        "v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2) \\, \\left(\\nabla f(w_t)\\right)^2\n",
        "$$\n",
        "\n",
        "3. **Bias Correction:**\n",
        "\n",
        "$$\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
        "$$\n",
        "\n",
        "4. **Nesterov Lookahead Term:**\n",
        "\n",
        "Instead of using \\\\(\\hat{m}_t\\\\) directly, Nadam uses a combination of the previous moment and the current gradient:\n",
        "\n",
        "$$\n",
        "m_{\\text{bar}} = \\frac{\\beta_1 \\, m_{t-1} + (1-\\beta_1) \\, \\nabla f(w_t)}{1-\\beta_1^t}\n",
        "$$\n",
        "\n",
        "5. **Parameter Update:**\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\, m_{\\text{bar}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\(w_t\\\\) is the parameter vector at iteration \\\\(t\\\\),\n",
        "- \\\\(\\eta\\\\) is the learning rate,\n",
        "- \\\\(\\beta_1\\\\) and \\\\(\\beta_2\\\\) are the decay rates for the first and second moments, and\n",
        "- \\\\(\\epsilon\\\\) is a small constant for numerical stability.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "Nadam (Nesterov-accelerated Adam) improves upon Adam by incorporating Nesterov’s accelerated gradient. Instead of updating with just the biased-corrected first moment \\\\(\\hat{m}_t\\\\), Nadam uses a lookahead estimate \\\\(m_{\\text{bar}}\\\\) that blends the previous momentum with the current gradient. This “lookahead” helps the optimizer to better anticipate the direction of the parameter update, often leading to faster convergence in practice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "exqNW0xsK3T0"
      },
      "outputs": [],
      "source": [
        "def nadam_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    Nadam update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
        "      - state: Dictionary to store moving averages for the first and second moments.\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "            \"lr\": learning rate,\n",
        "            \"beta1\": decay rate for the first moment,\n",
        "            \"beta2\": decay rate for the second moment,\n",
        "            \"eps\": small constant for numerical stability.\n",
        "      - **kwargs: Additional arguments (ignored for Nadam).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameter values.\n",
        "      - state: Updated state dictionary.\n",
        "    \"\"\"\n",
        "    lr    = hyperparams[\"lr\"]\n",
        "    beta1 = hyperparams[\"beta1\"]\n",
        "    beta2 = hyperparams[\"beta2\"]\n",
        "    eps   = hyperparams[\"eps\"]\n",
        "\n",
        "    if \"m\" not in state:\n",
        "        state[\"m\"] = torch.zeros_like(params)\n",
        "        state[\"v\"] = torch.zeros_like(params)\n",
        "        state[\"t\"] = 0\n",
        "\n",
        "    # Save the previous first moment (for the lookahead term).\n",
        "    m_prev = state[\"m\"].clone()\n",
        "    state[\"t\"] += 1\n",
        "\n",
        "    # Update first moment.\n",
        "    state[\"m\"] = beta1 * state[\"m\"] + (1 - beta1) * grad\n",
        "    # Update second moment.\n",
        "    state[\"v\"] = beta2 * state[\"v\"] + (1 - beta2) * grad**2\n",
        "\n",
        "    # Bias-corrected moments.\n",
        "    m_hat = state[\"m\"] / (1 - beta1 ** state[\"t\"])\n",
        "    v_hat = state[\"v\"] / (1 - beta2 ** state[\"t\"])\n",
        "\n",
        "    # Compute the lookahead (Nesterov) term:\n",
        "    m_bar = (beta1 * m_prev + (1 - beta1) * grad) / (1 - beta1 ** state[\"t\"])\n",
        "\n",
        "    # Update parameters.\n",
        "    new_params = params - lr * m_bar / (torch.sqrt(v_hat) + eps)\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AeitYOdKs_t",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for Nadam.\n",
        "nadam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Nadam.\n",
        "run_all_optimizations(update_func=nadam_update, hyperparams=nadam_hyperparams, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Nadam.\n",
        "nadam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Nadam.\n",
        "run_all_optimizations(update_func=nadam_update, hyperparams=nadam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "5sqnIdDMfg6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Nadam.\n",
        "nadam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Nadam.\n",
        "run_all_optimizations(update_func=nadam_update, hyperparams=nadam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "hPWgRn1CfhB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for Nadam.\n",
        "nadam_hyperparams = {\n",
        "    \"lr\": 0.001,      # Learning rate\n",
        "    \"beta1\": 0.9,     # Decay rate for first moment\n",
        "    \"beta2\": 0.999,   # Decay rate for second moment\n",
        "    \"eps\": 1e-8       # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using Nadam.\n",
        "run_all_optimizations(update_func=nadam_update, hyperparams=nadam_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "MPhMJgwNfhKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQEZwb6UR-_I"
      },
      "source": [
        "# Nadam Hyperparameter Analysis(5 Points)\n",
        "\n",
        "For the Nadam optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    nadam_hyperparams = {\n",
        "        \"lr\": 0.001,      # Learning rate\n",
        "        \"beta1\": 0.9,     # Decay rate for first moment\n",
        "        \"beta2\": 0.999,   # Decay rate for second moment\n",
        "        \"eps\": 1e-8       # Small constant for numerical stability\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various combinations of the learning rate, beta1, beta2, and epsilon values to analyze how they affect the performance of Nadam. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the values you tested for the learning rate, beta1, beta2, and epsilon. Provide a clear list or table of these parameter combinations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "| Learning Rate (LR) | Beta1 (β₁) | Beta2 (β₂) |\n",
        "|--------------------|------------|------------|\n",
        "| 0.05            | 0.95       | 0.999       |\n",
        "| 0.05            | 0.9        | 0.990       |\n",
        "| 0.001            | 0.95       | 0.990       |\n",
        "| 0.001            | 0.9        | 0.999       |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. **Performance Analysis:**\n",
        "   - How did different parameter combinations impact the convergence behavior of Nadam?\n",
        "   - Were there any specific combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested parameter combinations cause the optimizer to get trapped in local minima or exhibit other issues?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Learning Rate Effects:\n",
        "  - Higher LR (0.05): Enabled faster convergence but occasionally led to instability.\n",
        "  - Lower LR (0.001): Ensured stability but significantly slowed down optimization, making convergence inefficient.\n",
        "\n",
        "- Beta1 and Beta2 Effects:\n",
        "  - Lower β₁ (0.9) and β₂ (0.990): Allowed for more adaptive updates, improving responsiveness.\n",
        "  - Higher β₁ (0.95) and β₂ (0.999): Provided smoother updates but sometimes dampened optimization speed, especially with the lower learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of parameters achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Best combination: LR = 0.05, β₁ = 0.9, β₂ = 0.990\n",
        "\n",
        "This configuration offered the best balance of speed and stability, ensuring faster convergence without excessive oscillations or instability. The slightly lower decay rates (β₁ = 0.9, β₂ = 0.990) allowed for a more effective update mechanism.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying these hyperparameters. For example, did lower epsilon or higher beta values improve stability at the cost of slower convergence?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- A higher learning rate (0.05) improved convergence but increased the risk of instability with high β values.\n",
        "- A lower learning rate (0.001) was stable but too slow for practical optimization.\n",
        "- Higher decay rates (β₂ = 0.999) led to slower adaptation, while slightly lower values (β₂ = 0.990) improved responsiveness.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the learning rate, beta1, beta2, and epsilon when using Nadam in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "- Use LR = 0.05 for faster convergence but monitor for instability.\n",
        "- Set β₁ around 0.9 for a good balance between adaptability and stability.\n",
        "- Choose β₂ = 0.990 for effective weight updates without excessive damping.\n",
        "- If facing instability, slightly lower the learning rate or increase β values to smooth updates.\n",
        "\n",
        "---\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Nadam optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHZvVop1LB3W"
      },
      "source": [
        "# AdaDelta\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "AdaDelta adaptively adjusts the learning rate by using an exponentially decaying average of past squared gradients and squared updates. The update rules are as follows:\n",
        "\n",
        "1. **Accumulate Squared Gradients:**\n",
        "\n",
        "$$\n",
        "E[g^2]_t = \\rho \\, E[g^2]_{t-1} + (1-\\rho) \\, \\left(\\nabla f(w_t)\\right)^2\n",
        "$$\n",
        "\n",
        "2. **Compute the Parameter Update:**\n",
        "\n",
        "$$\n",
        "\\Delta w_t = - \\frac{\\sqrt{E[\\Delta w^2]_{t-1} + \\epsilon}}{\\sqrt{E[g^2]_t + \\epsilon}} \\, \\nabla f(w_t)\n",
        "$$\n",
        "\n",
        "3. **Update the Parameters:**\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t + \\Delta w_t\n",
        "$$\n",
        "\n",
        "4. **Accumulate Squared Updates:**\n",
        "\n",
        "$$\n",
        "E[\\Delta w^2]_t = \\rho \\, E[\\Delta w^2]_{t-1} + (1-\\rho) \\, \\left(\\Delta w_t\\right)^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\\\( \\rho \\\\) is the decay rate (typically around 0.95 or 0.9),\n",
        "- \\\\( \\epsilon \\\\) is a small constant for numerical stability,\n",
        "- \\\\( w_t \\\\) is the parameter vector at iteration \\\\( t \\\\),\n",
        "- \\\\( \\nabla f(w_t) \\\\) is the gradient at \\\\( w_t \\\\), and\n",
        "- \\\\( E[g^2]_t \\\\) and \\\\( E[\\Delta w^2]_t \\\\) are the exponentially decaying averages of squared gradients and updates, respectively.\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "AdaDelta is designed to overcome the aggressive learning rate decay problem of Adagrad by limiting the window of accumulated past gradients using an exponential decay. It does not require an explicit initial learning rate (though you may still set one if desired) and adapts the step size based on the history of gradients and updates. This makes AdaDelta particularly useful for problems where the optimal learning rate changes over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ORBNSW6oK45O"
      },
      "outputs": [],
      "source": [
        "def adadelta_update(params, grad, state, hyperparams, **kwargs):\n",
        "    \"\"\"\n",
        "    AdaDelta update function.\n",
        "\n",
        "    Parameters:\n",
        "      - params: Current parameter values (torch tensor).\n",
        "      - grad: Gradient of the loss with respect to params (torch tensor).\n",
        "      - state: Dictionary to store state variables.\n",
        "          Expected keys:\n",
        "            \"Eg2\": Exponential moving average of squared gradients.\n",
        "            \"Edw2\": Exponential moving average of squared updates.\n",
        "      - hyperparams: Dictionary of hyperparameters. Must contain:\n",
        "            \"rho\": Decay rate (e.g., 0.95),\n",
        "            \"eps\": A small constant for numerical stability.\n",
        "      - **kwargs: Additional arguments (ignored for AdaDelta).\n",
        "\n",
        "    Returns:\n",
        "      - new_params: Updated parameter values.\n",
        "      - state: Updated state dictionary.\n",
        "    \"\"\"\n",
        "    rho = hyperparams[\"rho\"]\n",
        "    eps = hyperparams[\"eps\"]\n",
        "\n",
        "    # Initialize state if not present.\n",
        "    if \"Eg2\" not in state:\n",
        "        state[\"Eg2\"] = torch.zeros_like(params)\n",
        "    if \"Edw2\" not in state:\n",
        "        state[\"Edw2\"] = torch.zeros_like(params)\n",
        "\n",
        "    # Update the exponential moving average of squared gradients.\n",
        "    state[\"Eg2\"] = rho * state[\"Eg2\"] + (1 - rho) * grad**2\n",
        "\n",
        "    # Compute the update:\n",
        "    # Note: Classic AdaDelta uses an effective learning rate of 1.\n",
        "    update = - (torch.sqrt(state[\"Edw2\"] + eps) / torch.sqrt(state[\"Eg2\"] + eps)) * grad\n",
        "\n",
        "    # Update parameters.\n",
        "    new_params = params + update\n",
        "\n",
        "    # Update the exponential moving average of squared updates.\n",
        "    state[\"Edw2\"] = rho * state[\"Edw2\"] + (1 - rho) * update**2\n",
        "\n",
        "    return new_params, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtC5EThVLDvq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters for AdaDelta.\n",
        "adadelta_hyperparams = {\n",
        "    \"rho\": 0.95,    # Decay rate\n",
        "    \"eps\": 1e-6     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using AdaDelta.\n",
        "run_all_optimizations(update_func=adadelta_update, hyperparams=adadelta_hyperparams, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for AdaDelta.\n",
        "adadelta_hyperparams = {\n",
        "    \"rho\": 0.90,    # Decay rate\n",
        "    \"eps\": 1e-6     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using AdaDelta.\n",
        "run_all_optimizations(update_func=adadelta_update, hyperparams=adadelta_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "9TtsYItrhq_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for AdaDelta.\n",
        "adadelta_hyperparams = {\n",
        "    \"rho\": 0.999,    # Decay rate\n",
        "    \"eps\": 1e-6     # Small constant for numerical stability\n",
        "}\n",
        "\n",
        "# Run optimizations for all benchmark functions using AdaDelta.\n",
        "run_all_optimizations(update_func=adadelta_update, hyperparams=adadelta_hyperparams, epochs=1000)"
      ],
      "metadata": {
        "id": "O23buyOzhrCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvX1PCYJSraw"
      },
      "source": [
        "# Adadelta Hyperparameter Analysis\n",
        "\n",
        "For the Adadelta optimizer, we have defined the initial hyperparameters as follows:\n",
        "\n",
        "    adadelta_hyperparams = {\n",
        "        \"rho\": 0.95,    # Decay rate\n",
        "        \"eps\": 1e-6     # Small constant for numerical stability\n",
        "    }\n",
        "\n",
        "In this section, you are required to test various values for the decay rate (rho) and epsilon to analyze how they affect the performance of Adadelta. Please answer the following questions based on your experiments:\n",
        "\n",
        "1. **Parameter Variation:**\n",
        "   - List all the values you tested for rho and epsilon. Provide a clear list or table of these parameter combinations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "| Decay Rate  |\n",
        "|-------------|\n",
        "| 0.90        |\n",
        "| 0.95        |\n",
        "| 0.990        |\n",
        "\n",
        "---\n",
        "\n",
        "2. **Performance Analysis:**\n",
        "   - How did different values of rho and epsilon impact the convergence behavior of Adadelta?\n",
        "   - Were there any specific parameter combinations that resulted in unstable behavior, overshooting, or slow convergence?\n",
        "   - Did any of the tested combinations cause the optimizer to get stuck in local minima or exhibit other issues?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "**Effect of Decay Rate**: A lower decay rate of 0.90 led to more responsive updates, which helped in dynamic optimization landscapes but introduced some instability. Increasing ρ to 0.95 provided a more balanced approach, maintaining stability while allowing sufficient adaptation. The highest decay rate, 0.999, resulted in smoother but slower updates, leading to more stable convergence at the cost of adaptability in complex functions.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Optimal Parameters:**\n",
        "   - Which combination of rho and epsilon achieved the best balance between convergence speed and stability?\n",
        "   - Explain why you consider these parameters optimal based on your observations.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "The decay rate of 0.95 emerged as the best choice, striking a balance between learning efficiency and stability. It prevented excessive noise while ensuring sufficient adaptation, making it a more reliable default option.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Trade-offs and Observations:**\n",
        "   - Discuss any trade-offs you noticed when varying rho and epsilon. For example, did a higher rho improve convergence stability at the cost of slower updates?\n",
        "   - Describe any patterns or trends that emerged from your experiments.\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "Lower decay rates improved responsiveness but made convergence noisier. Higher values helped maintain stability but slowed down optimization. Since AdaDelta does not require manual learning rate tuning, the primary tuning parameter was ρ, whose impact was relatively minor within the tested range. The optimizer performed well on simpler functions but struggled in more complex, multimodal landscapes.\n",
        "\n",
        "----\n",
        "\n",
        "5. **Recommendations:**\n",
        "   - Based on your experimental results, what recommendations would you give for selecting the decay rate and epsilon when using Adadelta in similar optimization tasks?\n",
        "\n",
        "### ✅ Answer\n",
        "\n",
        "A decay rate of 0.95 is generally preferable for AdaDelta as it provides a good balance of stability and adaptability. If the loss function fluctuates significantly, a lower value like 0.90 may help in faster adaptation. Conversely, for highly stable optimization scenarios, 0.999 may be considered to ensure smoother updates. However, for more challenging optimization problems, alternative adaptive optimizers like Adam or RMSprop may offer better results.\n",
        "\n",
        "---\n",
        "\n",
        "Ensure your report includes detailed observations, a comparative analysis, and a summary of all parameter combinations tested. Your analysis should clearly illustrate how the choice of these hyperparameters influences the performance of the Adadelta optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4VEzCEDS7kE"
      },
      "source": [
        "# Final Section\n",
        "\n",
        "I hope you enjoyed implementing these concepts. For the final submission, you are required to integrate your explanations regarding the algorithms discussed above. You simply need to elaborate on the points provided here.\n",
        "\n",
        "Best of luck!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}